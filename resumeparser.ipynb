{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da98312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "from pdf2image import convert_from_path\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece11d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\velam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726f17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Use your full Poppler bin path\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "images = convert_from_path(r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\Imgdownload2.pdf\", poppler_path=poppler_path)\n",
    "\n",
    "# Just to confirm\n",
    "for i, img in enumerate(images):\n",
    "    img.save(f\"page_{i+1}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9004455c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytesseract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tesseract_cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProgram Files\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTesseract-OCR\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtesseract.exe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m poppler_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRelease-24.08.0-0\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpoppler-24.08.0\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLibrary\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpytesseract\u001b[49m\u001b[38;5;241m.\u001b[39mpytesseract\u001b[38;5;241m.\u001b[39mtesseract_cmd \u001b[38;5;241m=\u001b[39m tesseract_cmd\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pytesseract' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up your paths correctly before running\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359e0789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting PDF pages to images...\n",
      "[INFO] Running OCR on page 1\n",
      "[INFO] Running OCR on page 2\n",
      "[INFO] Running OCR on page 3\n",
      "[INFO] Running OCR on page 4\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles from resume text...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " DIGVIJAYSINGHPARMAR\n",
      "\n",
      "Permanent Address: 40/C, Sangam Nagar, Indore, M.P â€” 452006\n",
      "Present Address:2195 NiralaKunj, Gas AthourityOf India Township\n",
      "Dibiyapur, Auraiya, Uttar Pradesh, |ndia-206244\n",
      "\n",
      "Emait:digv10sp@gmail.com, digvijay10sp@outlook.com\n",
      "Mobile: +91-9662489663\n",
      "\n",
      "in ee Jinkedin.com/in/digvijay-singh-parmar-aabb0953\n",
      "\n",
      "CAREER OBJECTIVE\n",
      "\n",
      "To utilize my ability and knowledge about Safety and Emergency control methods, including\n",
      "the detection of risks to prevent life-threatening harms and damages  \n",
      "\n",
      "Skills Found: ['mobilization', 'testing', 'control system', 'on', '18001', 'marital status', 'undertaking', 'specific job', 'de', 'listening', 'medium', 'schedule', 'pt', 'enhancement', 'outlook', 'driven', 'prevent', 'knowledge', 'oil and gas industries', 'oil and gas industry', 'specification', '2 years', 'gas industry', 'training modules', 'prospect', 'ltd', 'aspects', 'engineering', 'exercises', 'implementation', 'pvt', 'automatic', 'resource planning', 'people', 'carry out', 'organizing', 'methodologies', 'standard operating', 'globally', 'always', 'trainings', 'dynamic', 'psm', 'provide', 'certificate', 'maintaining', 'et', 'incident command', 'inspections & audits', 'operability', 'physical', 'providing', 'safety training', 'routine', 'plants', 'applicable', 'required resources', 's', 'disaster', 'sop preparation', 'nbc', 'fire safety audits', 'fire safety standards', 'any', 'senior', 'adequacy', 'book', 'specific requirements', 'drills', 'defence', 'french', 'examination', 'health safety', 'manufacturing', 'for', 'identifying', 'engineering practices', 'industrial', 'reliability', 'yearly', 'based', 'reports', 'resource', 'swimming', 'clean agent', 'good growth', 'codes', 'testing and operations', 'breathing', 'communications', 'action points', 'manager', 'maintenance reports', 'fire technology', 'authorization', 'provides', 'internal audit', 'mm', 'resources', 'industries', 'face', 'ips', 'sap', 'assist', 'inspection', 'profile', 'other', 'award', 'with', 'achieving', 'academic', 'breathing apparatus', 'mix', 'opito', 'ies', 'qra', 'steps', 'high velocity', 'jeeps', 'transportation', 'international', 'years', 'record', 'system', 'iso', 'confident', 'fire extinguishers', 'analysis', 'pump', 'maintenance work', 'personal', 'total', 'emergency calls', 'music', 'calls', 'requirements', 'position', 'release', 'valve', 'turnaround', 'planning', 'basis', 'providing recommendations', 'purchase requisitions', 'oh&s', 'communication equipment', 'fire hydrant system', 'certification', 'summary', 'industry', 'power plants', 'develop', 'school', 'accept', 'training', 'preparation', 'assessments', 'correctness', 'nfpa', 'coordination', 'client specification', 'required', 'extinguishers', 'protection', 'second', 'ee', 'command', 'objective', 'certificate of proficiency', 'mock drills', 'management', 'conducting', 'request', 'check', 'sheet', 'oil and gas', 'reading books', 'academic pursuits', 'work', 'testing and maintenance', 'ability', 'foam', 'make', 'is', 'pursuits', 'experience', 'sessions', 'conditions', 'proficiency in', 'risk assessment', 'purchase', 'relating', 'bureau', 'pm', 'senior officer', 'storage', 'national safety', 'ine', 'out', 'statutes', 'gaseous', 'up to', 'in', 'permanent address', 'fire safety', 'fire protection systems', 'and', 'status', 'hotels', 'fire & safety', 'standard operating procedures', 'division', 'material availability', 'department', 'qualitative', 'permanent', 'others', 'strive', 'a', 'permit request', 'easily', 'from', 'clean agent system', '2', 'in design', 'completion', 'stipulations', 'inventory', 'ensure', 'veritas', 'hst', 'emergency vehicles', 'mobile', 'organization', 'rie', 'responsible', 'review', 'no', 'measures', 'necessary', 'as', 'address', 'b', 'training department', 'within', 'c', 'emergency response', 'act', 'preparing', 'investigation of incidents', 'availability', 'round', 'procedures', 'points', 'expertise', 'crewmembers', 'carryout', 'clean', 'smile', 'successful completion', 'mnc', 'reference', 'allocation', 'nebosh', '2016', 'safety engineering', 'firefighting equipment', 'preparedness', 'power', 'b.s.', 'job', 'reliance', 'participate', 'worker', 'waste', 'iso 14001', 'incident', 'action', 'gas', 'use', 'preliminary investigation', '7', 'to', 'push', 'eee', 'international standards', 'module', 'documents', 'high rise', 'damages', 'interest', 'working experience', 'safe storage', 'believe', 'languages', 'checklist', 'employees', 'dcp', 'velocity', 'hazardous', 'extinguisher', 'design', 'course', 'id', 'emergency response exercises', 'co2', 'ai', 'com', 'new', 'ensure compliance', 'protection system', 'council', 'safety and emergency', 'versed', 'minimum', 'personnel', 'life', 'x', 'extra', 'declare', 'am', 'fire drills', 'spray', 'mee', 'maintain', 'designing', 'flooding', 'hindi', '24', 'planning and allocation', 'audit', 'its', 'executed', 'mig', 'excellent', 'bites', 'requisitions', 'reservation', '40', 't', 'pear', 'insure', 'water', 'methods', 'design and development', 'accidents', 'are', 'july', 'proficiency', '14001', 'modules', 'technique', 'fire protection system', 'cooking', 'agent', 'safety at work', 'safe', 'hse', 'reading', 'name', 'projects', 'technology', '1', 'ohsas', 'governments', 'apparatus', 'emergency', 'resource planning and allocation', 'utilize', 'systems', 'capable', 'declaration', 'improvements', 'birth', 'hard working', 'process', 'control', 'safety inspections', 'e', 'water pump', 'material master', 'process safety', 'protection systems', 'incident analysis', 'oe', 'log book', 'safety inspections & audits', 'firefighting', 'development', 'technical', 'corrective', 'modifications', 'career', 'present', 'operations', 'test', 'shop', 'inspections', 'can', 'master control', 'fire water', 'heat', 'well', 'assuring', 'due', 'seminar', 'environment', 'health', 'ready', 'response', 'investigation', 'specific', 'successful', 'fire', 'personality', 'recommendations', 'fire protection', 'ems', 'set', 'oh', 'days', 'information', 'date', 'oil', 'internal', 'organize', 'if', 'national safety council', 'high', 'fire safety inspection', 'advice', 'professional', 'shift activities', 'm', 'plant', 'inventory management', 'report', 'correct', 'technical expertise', 'control methods', 'english', 'executive', 'township', 'daily', 'incharge', 'updated', 'physical test', 'activities', 'travelling', 'gain', 'collage', 'operating procedures', 'factories', 'disposal', 'good engineering practices', 'recommend', 'evaluate', 'safety inspection', 'bachelor', 'site', 'additional responsibilities', 'program', 'code', 'upcoming projects', 'safety management', 'disaster management', 'maintenance', 'fire and safety audits', 'assessment', 'related', 'on site', 'buildings', 'conduct', 'deluge', 'sop', 'compliance', 'associated risks', 'provide training', 'properties', 'incidents', 'contractors', 'risks', 'loss', 'risk', 'self', 'skype', 'semi', 'heat detection', 'other shift', '9001', '91', 'observation', 'petrochemical', 'india', 'sit', 'working', 'safety professional', 'all', 'h', 'guidance', 'master', 'friends', 'running', 'academy', 'international requirements', 'national', 'always strive', 'plant level', 'hobbies', 'suggest', 'industrial safety', 'standard', 'immediate response', 'auditor', 'prevention', 'list', 'audits', 'ndia', 'high rise buildings', 'in life', 'smoke', 'mitigate', 'growth', 'iso 9001', 'new projects', '3d', 'up', 'for all', 'store', 'hard', 'participate in risk assessment', 'additional', '2d', 'civil', 'detection', 'national level', 'including', 'challenges', 'processes', 'equipment', 'limited', 'hazardous waste', 'log', 'critical', 'ra', 'safety standards', 'house', 'performance', 'hse training', 'university', 'standards', 'level', 'p', 'foam system', 'good', 'passport', 'practices', 'regular', 'at', 'mitigation', 'responsibilities', 'date of birth', 'books', 'portable', 'petrochemical plant', 'process safety management', 'global', 'also', 'measure', 'ims', 'fixed', 'station', 'manual', 'safety audits', 'operations and maintenance', 'gmail', 'fire safety inspections', 'rendering', 'rise', 'government', 'r', 'training schedule', 'vehicles', 'client', 'material', 'fire and safety', 'internal auditor', '2009', 'permit', 'per', 'documentations', 'operating', 'certifications', 'the set', 'working environment', 'safety', 'shift', 'communication', 'personal profile', 'responsibility', 'jobs', 'safety council', 'assure', 'carry', 'exhibit', 'officer', 'career objective', 'statute', 'of', 'motivated']\n",
      "Job Titles Found: ['testing', 'on', 'de', 'medium', 'schedule', 'pt', 'outlook', 'knowledge', '2 years', 'engineering', 'implementation', 'people', 'organizing', 'trainings', 'dynamic', 'physical', 's', 'senior', 'defence', 'french', 'manufacturing', 'industrial', 'reliability', 'reports', 'resource', 'communications', '.', 'manager', 'internal audit', 'mm', 'resources', 'ips', 'sap', 'inspection', 'other', 'academic', 'transportation', 'international', 'system', 'iso', 'analysis', 'personal', 'music', 'requirements', 'position', 'release', 'planning', 'basis', 'industry', 'school', 'training', 'ee', 'management', 'pursuits', 'purchase', 'pm', 'senior officer', 'storage', 'in', 'fire safety', 'division', 'permanent', 'a', '2', 'inventory', 'mobile', 'responsible', 'review', 'no', 'as', 'b', 'c', 'availability', 'procedures', 'carryout', 'mnc', 'reference', 'preparedness', 'power', 'job', 'worker', 'waste', 'incident', 'gas', '7', '15', 'languages', 'design', 'id', 'ai', 'b.e', 'immediate', 'com', 'personnel', 'life', 'x', 'extra', 'am', 'designing', 'flooding', 'hindi', '24', 'audit', 'mig', '40', 't', 'water', '14001', 'technique', 'cooking', 'agent', 'safe', 'hse', 'reading', 'projects', 'technology', '1', 'emergency', 'systems', 'process', 'control', 'e', 'development', 'technical', 'm.', 'operations', 'test', 'shop', 'environment', 'health', 'response', 'investigation', 'fire', 'recommendations', 'ems', 'oh', 'days', 'information', 'oil', 'internal', 'high', 'professional', 's.e', 'm', 'plant', 'inventory management', 'report', 'english', 'executive', 'activities', 'bachelor', 'program', 'maintenance', 'assessment', 'on site', 'conduct', 'compliance', 'my', 'noida', 'contractors', 'risk', 'skype', 'semi', 'india', 'safety professional', 'master', 'academy', 'national', 'auditor', 'prevention', 'growth', 'iso 9001', 'store', 'hard', 'civil', 'detection', 'processes', 'equipment', 'performance', 'hse training', 'p', 'regular', 'at', 'process safety management', 'global', 'ims', 'manual', 'government', 'r', 'client', 'material', 'internal auditor', 'safety', 'shift', 'communication', 'officer', 'motivated']\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    for kw in line.strip().split(\",\"):\n",
    "                        if kw.strip():\n",
    "                            keywords.add(kw.strip().lower())\n",
    "    return list(keywords)\n",
    "\n",
    "def extract_keywords(text, keywords):\n",
    "    found = []\n",
    "    for kw in keywords:\n",
    "        if re.search(r'\\b' + re.escape(kw) + r'\\b', text.lower()):\n",
    "            found.append(kw)\n",
    "    return found\n",
    "\n",
    "def process_resume(file_path, skills_folder, titles_folder):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Loading skills and job titles...\")\n",
    "    skills = load_keywords_from_folder(skills_folder)\n",
    "    titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "    print(\"[INFO] Extracting skills and titles from resume text...\")\n",
    "    found_skills = extract_keywords(text, skills)\n",
    "    found_titles = extract_keywords(text, titles)\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(\"Skills Found:\", found_skills)\n",
    "    print(\"Job Titles Found:\", found_titles)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": found_skills,\n",
    "        \"titles\": found_titles\n",
    "    }\n",
    "\n",
    "# === EXAMPLE USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\Imgdownload2.pdf\"  # <-- Replace with your file\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_skills_parts\"  # Folder with .txt/.csv skills\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_titles_parts\"  # Folder with .txt/.csv titles\n",
    "\n",
    "    result = process_resume(file_path, skills_folder, titles_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b51e75",
   "metadata": {},
   "source": [
    "(above)this is using re to match skills and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beeefca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.5.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Version (if available): 12.4\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version (if available):\", torch.version.cuda)\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7258ceb",
   "metadata": {},
   "source": [
    "below using spacy just some trail not using dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf261d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed3386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Job Titles: []\n",
      "Extracted Skills: ['problem solving', 'project management', 'customer service', 'communication', 'marketing', 'teamwork', 'sales', 'patient care', 'leadership']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of predefined job titles for comparison\n",
    "job_titles_db = [\n",
    "    'licensed assistant salon manager', 'store manager', 'nurse', 'software engineer', 'data analyst', \n",
    "    'project manager', 'sales manager', 'mechanical engineer', 'hr manager', 'safety officer', \n",
    "    'maintenance manager', 'senior engineer', 'research assistant', 'chief technology officer', \n",
    "    'civil engineer', 'product manager', 'marketing manager', 'accounting manager'\n",
    "    # Add more job titles as required\n",
    "]\n",
    "\n",
    "# Example list of predefined skills (expand this list as needed)\n",
    "skills_db = [\n",
    "    'customer service', 'communication', 'problem solving', 'teamwork', 'leadership', \n",
    "    'project management', 'data analysis', 'marketing', 'strategic planning', 'coding', 'software development',\n",
    "    'nursing', 'patient care', 'sales', 'negotiation', 'time management', 'critical thinking'\n",
    "    # Add more skills as required\n",
    "]\n",
    "\n",
    "# Function to extract job titles using spaCy NER and custom rules\n",
    "def extract_job_titles(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Define the custom job title patterns (using tokens like 'manager', 'engineer', etc.)\n",
    "    job_title_keywords = ['Manager', 'engineer', 'assistant', 'director', 'supervisor', 'specialist', 'officer']\n",
    "    \n",
    "    # Extract NER entities that might be job titles (check for relevant keywords)\n",
    "    job_titles = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PERSON']:  # Consider organization names or people for titles\n",
    "            if any(keyword in ent.text.lower() for keyword in job_title_keywords):\n",
    "                job_titles.append(ent.text.strip())\n",
    "    \n",
    "    # Filter out duplicates and return\n",
    "    return list(set(job_titles))\n",
    "\n",
    "# Function to extract skills using regex based matching or predefined list\n",
    "def extract_skills(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract skills based on the predefined list\n",
    "    extracted_skills = []\n",
    "    for skill in skills_db:\n",
    "        if re.search(r'\\b' + re.escape(skill) + r'\\b', text, re.IGNORECASE):\n",
    "            extracted_skills.append(skill)\n",
    "    \n",
    "    # Filter out duplicates and return\n",
    "    return list(set(extracted_skills))\n",
    "\n",
    "# Sample text input\n",
    "text_input = \"\"\"\n",
    "Licensed Assistant Salon Manager with 5 years of experience. I specialize in customer service, communication, \n",
    "teamwork, and problem solving. I have also worked as a Store Manager for 3 years. Experienced with sales, marketing, \n",
    "and project management. Previously, I worked as a Nurse, focusing on patient care and leadership in healthcare settings.\n",
    "\"\"\"\n",
    "\n",
    "# Extract job titles and skills from the input text\n",
    "job_titles = extract_job_titles(text_input)\n",
    "skills = extract_skills(text_input)\n",
    "\n",
    "print(\"Extracted Job Titles:\", job_titles)\n",
    "print(\"Extracted Skills:\", skills)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59206685",
   "metadata": {},
   "source": [
    "using spacy phrasematcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0c2808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading DOCX file...\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " Velamala Pavan Krishna\n",
      "\n",
      "SUMMARY\n",
      "Passionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\n",
      "\n",
      "Phone:\n",
      "+91 7569637875\n",
      "Email:\n",
      "velamalapavankrishna@gmail.com\n",
      "Address: Visakhapatnam , \n",
      "Andhra Pradesh , India\n",
      "    530046\n",
      "\n",
      "Profile links\n",
      "\n",
      "Linkedin: linkedin.co \n",
      "\n",
      "Skills Found: ['and', 'strong foundation', 'Language', 'IBM', 'C', 'symptoms', 'management system', 'emerging technologies', 'Collaboration', 'technologies', 'HealthCare AI', 'HealthCare', 'Database', 'model', 'computer', 'Tools', 'Data Structures', 'making', 'Mar', 'Notebook', 'Azure', 'reservation', 'ML', 'computer science', 'of', 'in', 'Using', 'Mental', 'Python', '7', 'RISK REDUCTION', 'Data', 'Flexible', 'learning', '2024', 'Microsoft Certified', 'Azure AI', 'Google', 'Data Structures and Algorithms', 'Generative AI', 'Solving', 'CERTIFICATIONS', 'EDUCATION', 'by', 'Eager', 'socket', 'Team', 'data analysis', 'emerging', 'Microsoft', 'name', 'with', 'CS', 'Github', 'Email', 'Server', 'team', 'fundamental', 'Technical skills', 'analysis', 'Mental health', 'Committed to continuous learning', 'scratch', 'reservation system', 'PyTorch', 'prediction', 'Soft skills:', 'Thinking', 'Core', 'impact', 'Member', '2023', 'Certified', 'Hack', 'management', 'Generative', 'ON', 'world', 'Flask', 'data', 'html', 'meaningful impact', 'Challenge', 'John', 'Problem', 'VScode', 'Development', 'Club', 'July', 'Programming', 'DNS', 'Linkedin', 'Models', 'Passionate', 'serving', 'machine', 'Adaptive', 'Tools:', 'current', 'Soft skills', 'on', 'continuous learning', 'a', 'Tech', 'Adaptive learning', 'AI', 'py', 'logistic', 'Web Developers', 'Dec', 'Web Development', 'Javascript', 'food serving', 'Structures', 'css', 'programming', 'using', 'Phone', 'Artificial Intelligence', 'creative', 'collaboration', 'CONFERENCE', 'RESILIENCE', 'India', 'SUMMARY', 'Library management system', 'Developers', 'University', 'problems', 'system', 'skills', 'EXPERIENCE', 'for', 'Critical Thinking', 'Language Models', 'swing', 'game', 'health', 'REDUCTION', 'Intelligence', 'Landmark', 'Profile', 'building', 'creative solutions', 'Data Science', 'Seminar', 'meaningful', 'AI Fundamentals', 'java', 'foundation', 'ACTIVITIES', 'Technical', 'Technical skills:', 'HTML', 'MySQL', 'detection', 'Critical', 'solutions', 'Diabetes', 'ongoing', 'project', 'Iris', 'Communication', 'Jupyter Notebook', 'Large', 'Jupyter', '9', 'at', 'Science', 'Matlab', 'System', 'skills:', 'Address', 'to', 'Fundamentals', 'hackathon', 'Library', 'Git', 'Hopkins', 'Java', 'Soft', 'food', 'strong', 'explore', 'PROJECTS', 'from', 'Operating System', 'Bus', 'DeepLearning', 'CSS', 'chatbot', 'continuous', 'B', 'based', 'Interact', 'real', 'student', 'Computer Science', 'links', 'Chess', 'Large Language Models', 'javascript', 'Algorithms', 'RISK', 'GPA', 'Committed', 'assembler', 'science', 'machine learning', 'Skills', 'Proxy', 'Computer', 'apply', 'Working', 'model building', 'Library management', 'Team Collaboration', 'INTERNATIONAL', 'OOPS', 'Operating', 'Web']\n",
      "Job Titles Found: ['Language', 'IBM', 'C', 'Collaboration', 'HealthCare', 'Database', 'model', 'computer', 'Tools', 'Azure', 'ML', 'computer science', 'in', 'Mental', 'Python', '7', 'Data', 'Flexible', 'learning', '2024', 'George', 'Google', 'Generative AI', 'EDUCATION', 'by', 'data analysis', 'Microsoft', 'CS', 'Github', 'Email', 'Server', 'analysis', 'Mental health', 'scratch', 'PyTorch', 'impact', 'Certified', 'management', 'ON', 'data', 'html', 'Problem', '71', 'Development', 'Programming', 'DNS', 'Linkedin', 'Models', 'machine', '70', 'on', 'a', 'Tech', 'AI', 'logistic', 'Web Development', 'Web Developers', 'Javascript', '4', 'Structures', 'css', 'programming', 'Artificial Intelligence', 'creative', 'collaboration', 'India', 'system', 'Moody', 'swing', 'health', 'Data Science', 'building', 'java', 'Technical', 'ACTIVITIES', 'HTML', 'MySQL', 'detection', 'solutions', 'Diabetes', 'ongoing', 'project', 'Communication', 'Large', '9', 'at', 'Science', 'Matlab', 'System', '.', 'Git', 'Java', 'food', 'strong', 'PROJECTS', 'Bus', 'CSS', 'chatbot', 'B', 'student', 'Computer Science', 'javascript', 'Algorithms', 'RISK', 'assembler', 'science', 'machine learning', 'Proxy', 'Computer', '23', 'INTERNATIONAL', 'Web']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "\n",
    "# Set your poppler path (update accordingly if needed)\n",
    "poppler_path = r\"C:\\path\\to\\poppler\\bin\"  # Replace with your poppler path\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    for kw in line.strip().split(\",\"):\n",
    "                        kw_clean = kw.strip()\n",
    "                        if kw_clean:\n",
    "                            keywords.add(kw_clean.lower())\n",
    "    return list(keywords)\n",
    "\n",
    "def create_phrase_matcher(keywords, label):\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = [nlp.make_doc(text) for text in keywords]\n",
    "    matcher.add(label, patterns)\n",
    "    return matcher\n",
    "\n",
    "def extract_keywords_with_spacy(text, skills_keywords, titles_keywords):\n",
    "    doc = nlp(text)\n",
    "    skills_matcher = create_phrase_matcher(skills_keywords, \"SKILL\")\n",
    "    titles_matcher = create_phrase_matcher(titles_keywords, \"TITLE\")\n",
    "\n",
    "    skills_found = set()\n",
    "    titles_found = set()\n",
    "\n",
    "    for match_id, start, end in skills_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        skills_found.add(span.text)\n",
    "\n",
    "    for match_id, start, end in titles_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        titles_found.add(span.text)\n",
    "\n",
    "    return list(skills_found), list(titles_found)\n",
    "\n",
    "def process_resume(file_path, skills_folder, titles_folder):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Loading skills and job titles...\")\n",
    "    skills = load_keywords_from_folder(skills_folder)\n",
    "    titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "    print(\"[INFO] Extracting skills and titles using spaCy...\")\n",
    "    found_skills, found_titles = extract_keywords_with_spacy(text, skills, titles)\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(\"Skills Found:\", found_skills)\n",
    "    print(\"Job Titles Found:\", found_titles)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": found_skills,\n",
    "        \"titles\": found_titles\n",
    "    }\n",
    "\n",
    "# === EXAMPLE USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\Users\\velam\\OneDrive\\Documents\\pavan's resume.docx\"\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_skills_parts\"\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_titles_parts\"\n",
    "\n",
    "    result = process_resume(file_path, skills_folder, titles_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3fa42",
   "metadata": {},
   "source": [
    "this is using spacy phrasematcher and lemmetizing skills and titles ( dont use it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af5e3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting PDF pages to images...\n",
      "[INFO] Running OCR on page 1\n",
      "[INFO] Running OCR on page 2\n",
      "[INFO] Running OCR on page 3\n",
      "[INFO] Running OCR on page 4\n",
      "Lemmatizing\n",
      "skill extraction...\n",
      "titles extraction..\n",
      "Lemmatizing skills...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 115\u001b[0m\n\u001b[0;32m    111\u001b[0m skills_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mamrita_uni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProjects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124masmacs internship\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrecognition of skills and title\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124munique_job_skills_parts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m titles_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mamrita_uni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProjects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124masmacs internship\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrecognition of skills and title\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124munique_job_titles_parts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 115\u001b[0m skills, titles \u001b[38;5;241m=\u001b[39m \u001b[43mextract_skills_and_titles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskills_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Skills:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(skills))\n",
      "Cell \u001b[1;32mIn[2], line 91\u001b[0m, in \u001b[0;36mextract_skills_and_titles\u001b[1;34m(file_path, skills_folder, titles_folder)\u001b[0m\n\u001b[0;32m     89\u001b[0m raw_titles \u001b[38;5;241m=\u001b[39m load_terms_from_folder(titles_folder)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLemmatizing skills...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m lemmatized_skills \u001b[38;5;241m=\u001b[39m {\u001b[43mlemmatize_phrase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m raw_skills}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLemmatizing titles..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m lemmatized_titles \u001b[38;5;241m=\u001b[39m {lemmatize_phrase(term) \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m raw_titles}\n",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m, in \u001b[0;36mlemmatize_phrase\u001b[1;34m(phrase)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_phrase\u001b[39m(phrase):\n\u001b[1;32m---> 50\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_alpha])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\pipeline\\transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\pipeline\\transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\ml\\tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[1;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\ml\\parser_model.pyx:250\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[1;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\layers\\with_array.py:36\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     33\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_ragged_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\layers\\with_array.py:91\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ragged_forward\u001b[39m(\n\u001b[0;32m     88\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     90\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 91\u001b[0m     Y, get_dX \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataXd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYr: Ragged) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ragged:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[38;5;241m.\u001b[39mdataXd), dYr\u001b[38;5;241m.\u001b[39mlengths)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\layers\\maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[1;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[0;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pytesseract\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pdf2image import convert_from_path\n",
    "from docx import Document\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Set your poppler path (update accordingly if needed)\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# --- FILE LOADER (Skills/Titles) ---\n",
    "\n",
    "def load_terms_from_folder(folder_path):\n",
    "    terms = set()\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.txt') or file.endswith('.csv'):\n",
    "            with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    items = re.split(r',|;', line.strip())\n",
    "                    for item in items:\n",
    "                        cleaned = item.strip().lower()\n",
    "                        if cleaned:\n",
    "                            terms.add(cleaned)\n",
    "    return terms\n",
    "\n",
    "def lemmatize_phrase(phrase):\n",
    "    doc = nlp(phrase.lower())\n",
    "    return ' '.join([token.lemma_ for token in doc if token.is_alpha])\n",
    "\n",
    "# --- MATCHER SETUP ---\n",
    "\n",
    "def build_matcher(nlp, terms):\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    patterns = [nlp.make_doc(term) for term in terms]\n",
    "    matcher.add(\"TERM_MATCH\", patterns)\n",
    "    return matcher\n",
    "\n",
    "def extract_matched_terms(text, matcher):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matched = set()\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        matched.add(span.text.lower())\n",
    "    return list(matched)\n",
    "\n",
    "# --- MAIN PROCESS ---\n",
    "\n",
    "def extract_skills_and_titles(file_path, skills_folder, titles_folder):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only PDF and DOCX supported.\")\n",
    "\n",
    "    # Preprocess\n",
    "    print(\"Lemmatizing\")\n",
    "    lemmatized_text = lemmatize_phrase(text)\n",
    "\n",
    "    # Load terms\n",
    "    print(\"skill extraction...\")\n",
    "    raw_skills = load_terms_from_folder(skills_folder)\n",
    "    print(\"titles extraction..\")\n",
    "    raw_titles = load_terms_from_folder(titles_folder)\n",
    "    print(\"Lemmatizing skills...\")\n",
    "    lemmatized_skills = {lemmatize_phrase(term) for term in raw_skills}\n",
    "    print(\"Lemmatizing titles..\")\n",
    "    lemmatized_titles = {lemmatize_phrase(term) for term in raw_titles}\n",
    "\n",
    "    # Build matchers\n",
    "    print(\"building matcher...\")\n",
    "    skills_matcher = build_matcher(nlp, lemmatized_skills)\n",
    "    titles_matcher = build_matcher(nlp, lemmatized_titles)\n",
    "    print(\"matching terms..\")\n",
    "    # Match terms\n",
    "    matched_skills = extract_matched_terms(lemmatized_text, skills_matcher)\n",
    "    matched_titles = extract_matched_terms(lemmatized_text, titles_matcher)\n",
    "\n",
    "    return sorted(set(matched_skills)), sorted(set(matched_titles))\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual paths\n",
    "    file_path = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\Imgdownload2.pdf\"\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_skills_parts\"\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_titles_parts\"\n",
    " \n",
    "\n",
    "    skills, titles = extract_skills_and_titles(file_path, skills_folder, titles_folder)\n",
    "\n",
    "    print(\"Extracted Skills:\")\n",
    "    print(list(skills))\n",
    "\n",
    "    print(\"\\nExtracted Job Titles:\")\n",
    "    print(list(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b44a5a",
   "metadata": {},
   "source": [
    "dont embark on the path of lemmetinzing skills and titles, it took 500 mins and yet skills were too large to be lemmetized "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ea525",
   "metadata": {},
   "source": [
    "below is using same spacy code on new dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecfb13b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading DOCX file...\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " Velamala Pavan Krishna\n",
      "\n",
      "SUMMARY\n",
      "Passionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\n",
      "\n",
      "Phone:\n",
      "+91 7569637875\n",
      "Email:\n",
      "velamalapavankrishna@gmail.com\n",
      "Address: Visakhapatnam , \n",
      "Andhra Pradesh , India\n",
      "    530046\n",
      "\n",
      "Profile links\n",
      "\n",
      "Linkedin: linkedin.co \n",
      "\n",
      "Skills Found: ['collaboration', 'Matlab', 'project', 'management system', 'Collaboration', 'Algorithms', 'Adaptive learning', 'Web Development', 'Developers', 'data analysis', 'Azure', 'RISK', 'html', 'scratch', 'Microsoft', 'Data Science', 'CS', 'building', 'Java', 'Fundamentals', 'HealthCare', 'ML', 'C', 'Mental health', 'computer science', 'Soft skills', 'MySQL', 'assembler', 'Email', 'machine learning', 'css', 'learning', 'Bus', 'Python', 'CSS', 'Linkedin', 'Critical Thinking', 'Iris', 'Chess', 'java', 'foundation', 'Intelligence', 'Google', 'Mar', 'emerging technologies', 'health', 'RISK REDUCTION', 'Artificial Intelligence', 'Javascript', 'javascript', 'Web', 'Database', 'IBM', 'Models', 'py', 'Library management', 'model building', 'Diabetes', 'Proxy', 'Github', 'Flask', 'HTML', 'Computer Science', 'prediction', 'Git']\n",
      "Job Titles Found: ['assembler', 'classifier', 'student']\n",
      "[INFO] Converting PDF pages to images...\n",
      "[INFO] Running OCR on page 1\n",
      "[INFO] Running OCR on page 2\n",
      "[INFO] Running OCR on page 3\n",
      "[INFO] Running OCR on page 4\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " DIGVIJAYSINGHPARMAR\n",
      "\n",
      "Permanent Address: 40/C, Sangam Nagar, Indore, M.P â€” 452006\n",
      "Present Address:2195 NiralaKunj, Gas AthourityOf India Township\n",
      "Dibiyapur, Auraiya, Uttar Pradesh, |ndia-206244\n",
      "\n",
      "Emait:digv10sp@gmail.com, digvijay10sp@outlook.com\n",
      "Mobile: +91-9662489663\n",
      "\n",
      "in ee Jinkedin.com/in/digvijay-singh-parmar-aabb0953\n",
      "\n",
      "CAREER OBJECTIVE\n",
      "\n",
      "To utilize my ability and knowledge about Safety and Emergency control methods, including\n",
      "the detection of risks to prevent life-threatening harms and damages  \n",
      "\n",
      "Skills Found: ['Skype', 'steps', 'contractors', 'Conducting', 'd', 'Firefighting', 'SAP', 'power plants', 'petrochemical', 'international standards', 'completion', 'store', 'Purchase Requisitions', 'Water', 'Requisitions', 'preparation', 'Process Safety', 'reliability', 'working environment', 'observation', 'prospect', 'HST', 'LTD', 'English', 'buildings', 'Gas Industry', 'Risk', 'ISO 14001', 'Hindi', 'Smoke', 'Training', 'Swimming', 'Velocity', 'record', 'Foam', 'x', 'Gas', 'high rise', 'COLLAGE', 'VERITAS', 'Codes', 'WORKING EXPERIENCE', 'risk assessment', 'oe', 'prevention', 'Languages', 'Fire protection', 'Sit', 'mitigation', 'Process Safety Management', 'mock', 'Heat', 'IPS', 'award', 'master control', 'NFPA', 'protection', 'training', 'investigation', 'cooking', 'availability', 'SAFETY ENGINEERING', 'Birth', 'measures', 'Rendering', 'ISO 9001', 'communication equipment', 'water', 'smoke', 'Safety Training', 'French', 'DCP', 'modifications', 'Safety training', 'TECHNOLOGY', 'authorization', 'waste', 'conducting', 'Health', 'Status', 'safety training', 'Preparation', 'operations', 'Fire Safety', 'design', 'communications', 'ee', 'statutes', 'risk', 'ISO', 'Hobbies', 'fire safety', 'testing', 'on site', 'Oil', 'internal audit', 'PSM', 'OH&S', 'Passport', 'Running', 'organization', 'Mobile', 'protection systems', 'firefighting', 'plants', 'Manufacturing', 'mobilization', 'disposal', 'EMS', 'C', 'assessment', 'Rie', 'Mig', 'command', 'foam', 'CO2', 'maintenance', 'Government', 'INDUSTRIAL SAFETY', 'fire protection', 'incident command', 'preparedness', 'GAS', 'pear', 'storage', 'transportation', 'NEBOSH', 'Technology', 'Spray']\n",
      "Job Titles Found: ['SENIOR OFFICER', 'Internal Auditor', 'Safety professional']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "\n",
    "# Set your poppler path (update accordingly if needed)\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\"\"\"\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    for kw in line.strip().split(\",\"):\n",
    "                        kw_clean = kw.strip()\n",
    "                        if kw_clean:\n",
    "                            keywords.add(kw_clean.lower())\n",
    "    return list(keywords)\n",
    "\"\"\"\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    # Don't split on commas if you want multi-word phrases\n",
    "                    kw_clean = line.strip().lower()\n",
    "                    if kw_clean:\n",
    "                        keywords.add(kw_clean)\n",
    "    return list(keywords)\n",
    "\n",
    "def create_phrase_matcher(keywords, label):\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = [nlp.make_doc(text) for text in keywords]\n",
    "    matcher.add(label, patterns)\n",
    "    return matcher\n",
    "\n",
    "def extract_keywords_with_spacy(text, skills_keywords, titles_keywords):\n",
    "    doc = nlp(text)\n",
    "    skills_matcher = create_phrase_matcher(skills_keywords, \"SKILL\")\n",
    "    titles_matcher = create_phrase_matcher(titles_keywords, \"TITLE\")\n",
    "\n",
    "    skills_found = set()\n",
    "    titles_found = set()\n",
    "\n",
    "    for match_id, start, end in skills_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        skills_found.add(span.text)\n",
    "\n",
    "    for match_id, start, end in titles_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        titles_found.add(span.text)\n",
    "\n",
    "    return list(skills_found), list(titles_found)\n",
    "\n",
    "def process_resume(file_path, skills_folder, titles_folder):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Loading skills and job titles...\")\n",
    "    skills = load_keywords_from_folder(skills_folder)\n",
    "    titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "    print(\"[INFO] Extracting skills and titles using spaCy...\")\n",
    "    found_skills, found_titles = extract_keywords_with_spacy(text, skills, titles)\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(\"Skills Found:\", found_skills)\n",
    "    print(\"Job Titles Found:\", found_titles)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": found_skills,\n",
    "        \"titles\": found_titles\n",
    "    }\n",
    "\n",
    "# === EXAMPLE USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\Users\\velam\\OneDrive\\Documents\\pavan's resume.docx\"\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\skills\"\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\titles\"\n",
    "    file_path2= r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\Imgdownload2.pdf\"\n",
    "    result = process_resume(file_path, skills_folder, titles_folder)\n",
    "    result=process_resume(file_path2, skills_folder, titles_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f8269",
   "metadata": {},
   "source": [
    "spacy ner model with out custom model trail on skills and titles for above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b84d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading DOCX file...\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " Velamala Pavan Krishna\n",
      "\n",
      "SUMMARY\n",
      "Passionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\n",
      "\n",
      "Phone:\n",
      "+91 7569637875\n",
      "Email:\n",
      "velamalapavankrishna@gmail.com\n",
      "Address: Visakhapatnam , \n",
      "Andhra Pradesh , India\n",
      "    530046\n",
      "\n",
      "Profile links\n",
      "\n",
      "Linkedin: linkedin.co \n",
      "\n",
      "Skills Found: ['Library management', 'html', 'Microsoft', 'py', 'Data Science', 'Web Development', 'Data Structures', 'Algorithms', 'C', 'VScode', 'data analysis', 'IBM', 'ICTR-3 (INTERNATIONAL CONFERENCE ON TSUNAMI RISK REDUCTION& RESILIENCE', 'AI Fundamentals', 'collaboration', 'MySQL', 'scratch', 'machine learning', 'Developers', 'Chess', 'Proxy', 'B-Tech', 'Database', 'CERTIFICATIONS', 'Javascript', 'Intelligence', 'Using Python to Interact', 'assembler', 'building', 'Web', 'emerging technologies', 'learning', 'Email', 'prediction', 'Mar', 'PyTorch\\nMental', 'model building', 'Azure', 'Linkedin', 'AI & Development', 'DeepLearning', 'Artificial Intelligence', 'computer science', 'Flask', 'Andhra Pradesh', 'Models', 'Diabetes', 'health', 'Google', 'Git', 'Matlab', 'Python for Data Science', 'java', 'Collaboration', 'Skills\\nTechnical', 'HealthCare', 'CSS', 'Critical Thinking', 'Github', 'AI Club', 'HTML', 'foundation', 'GPA', 'Fundamentals', 'javascript', 'Java', 'AI', 'management system', 'Problem-Solving      Team Collaboration \\n Communication       Critical Thinking\\nFlexible Adaptive', 'Adaptive learning', 'Python', 'John Hopkins University', 'Mental health', 'css', 'Computer Science', 'RISK', 'RISK REDUCTION', 'CS', 'Iris', 'PhysioNet Challenge', 'project', 'Soft skills', 'ML', 'Bus']\n",
      "Job Titles Found: ['classifier', 'student', 'assembler']\n",
      "[INFO] Converting PDF pages to images...\n",
      "[INFO] Running OCR on page 1\n",
      "[INFO] Running OCR on page 2\n",
      "[INFO] Running OCR on page 3\n",
      "[INFO] Running OCR on page 4\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " DIGVIJAYSINGHPARMAR\n",
      "\n",
      "Permanent Address: 40/C, Sangam Nagar, Indore, M.P â€” 452006\n",
      "Present Address:2195 NiralaKunj, Gas AthourityOf India Township\n",
      "Dibiyapur, Auraiya, Uttar Pradesh, |ndia-206244\n",
      "\n",
      "Emait:digv10sp@gmail.com, digvijay10sp@outlook.com\n",
      "Mobile: +91-9662489663\n",
      "\n",
      "in ee Jinkedin.com/in/digvijay-singh-parmar-aabb0953\n",
      "\n",
      "CAREER OBJECTIVE\n",
      "\n",
      "To utilize my ability and knowledge about Safety and Emergency control methods, including\n",
      "the detection of risks to prevent life-threatening harms and damages  \n",
      "\n",
      "Skills Found: ['SAP', 'storage', 'Health Safety at Work', 'PVT LTD', 'NEBOSH', 'Velocity Water Spray System', 'Safety & Loss', 'IMS Internal Auditor Certificate', 'store', 'QRA', 'plants', 'testing', 'LTD', 'Fire Safety', 'incident command', 'mock', 'Achieving Excellent Position', 'reliability', 'ee', 'waste', 'completion', 'C', 'Hindi', 'Suggest', 'SAP-PM & SAP-MM\\n\\n', 'Smoke', 'Develop/Evaluate', 'conducting', 'Reliance Physical Test Examination', 'Hindi\\n\\nPermanent Address', 'communication equipment', 'pear', 'ISO 14001', 'Water', 'EMS', 'Languages', 'Velocity', 'Smoke & Heat Detection\\nSystem', 'IPS', 'Fire Safety Inspections & Audits/ Risk\\nAssessments', 'OFFICER - FIRE & SAFETY', 'organization', 'Marital Status: Married\\n\\nLanguages :', 'on site', 'Safe', 'Manufacturing', 'master control', 'NAGPUR', 'SAFETY ENGINEERING', 'Hobbies', 'protection', 'Health', 'mitigation', 'safety training', 'Status', 'working environment', 'IMS', 'Spray', 'measures', 'ISO', 'protection systems', 'HST', 'Mobile', 'preparation', 'TECHNOLOGY', 'WORKING EXPERIENCE', 'French', 'contractors', 'availability', 'Requisitions', 'ISO 14001 & 18001', 'power plants', 'Rie', 'Fire Safety Audits', 'Purchase Requisitions', 'Heat', 'CO2 Flooding', 'Fire protection', 'award', 'INDUSTRIAL SAFETY', 'authorization', 'smoke', 'B.E) Fire Technology', 'foam', 'Fire and Safety', 'Material', 'transportation', 'NFPA', 'high rise', 'NBC Code', 'CO2', 'fire safety', 'Maintain', 'OHSAS-18001', 'NATIONAL\\nCIVIL DEFENCE COLLAGE', 'risk assessment', 'Sit', 'cooking', 'Safety IES IPS ACADEMY', 'statutes', 'firefighting', 'Process Safety', 'SCHOOL', 'Foam', 'VERITAS', 'Firefighting', 'DCP', 'd', 'Safety training', 'Semi Fixed Foam System', 'Preparation', 'assessment', 'Mig', 'Process Safety Management', 'international standards', 'ISO 9001', 'Carryout', 'COLLAGE', 'GAS', 'design', 'Birth', 'VIJAYNAGAR INDORE Lette\\nPERSONALITY TRAIT', 'Oil', 'Participate', 'Hydrant System', 'Undergone 2D+3D Designing', 'prevention', 'Training', 'SAP-PM & SAP-MM(such', 'prospect', 'RA', 'Purchase Requisitions & Material', 'Gaseous Flooding System', 'water', 'Technology', 'OH&S', 'internal audit', 'Present Address', 'modifications', 'preparedness', 'Passport', 'petrochemical', 'HSE', 'contro', 'INDORE', 'oe', 'ISO-14001', 'Gas Industry', 'Gas', 'mobilization', 'x', 'steps', 'Swimming', 'Oil and Gas', 'disposal', 'Running', 'IES IPS ACADEMY', 'Rendering', 'BOCW', 'operations', 'NFPA\\nCodes', 'Government', 'training', 'investigation', 'Fire & Safety', 'OISD', 'RELIANCE INDUSTRIES LIMITED', 'maintenance', 'fire protection', 'NATIONAL SAFETY COUNCIL', 'command', 'Dahej-Manufacturing Division', 'Risk', 'Conducting', 'Uttar Pradesh', 'observation', 'CERTIFICATIONS& EXTRA- CURICULLAR ACTIVITES', 'NiralaKunj', 'communications', 'Swimming/Running/Chin-Up/Sit-Up/Push-Up', 'GOVT', 'Skype', 'Safety Training', 'Fire Hydrant System', 'PSM', 'English', 'Codes', 'record', 'buildings', 'risk']\n",
      "Job Titles Found: ['Safety professional', 'SENIOR OFFICER', 'FIRE TECHNOLOGY & SAFETY ENGINEERING', 'Internal Auditor']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "\n",
    "# Set your poppler and tesseract paths\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    kw_clean = line.strip().lower()\n",
    "                    if kw_clean:\n",
    "                        keywords.add(kw_clean)\n",
    "    return list(keywords)\n",
    "\n",
    "def create_phrase_matcher(keywords, label):\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = [nlp.make_doc(text) for text in keywords]\n",
    "    matcher.add(label, patterns)\n",
    "    return matcher\n",
    "\n",
    "def extract_keywords_with_spacy(text, skills_keywords, titles_keywords):\n",
    "    doc = nlp(text)\n",
    "    skills_matcher = create_phrase_matcher(skills_keywords, \"SKILL\")\n",
    "    titles_matcher = create_phrase_matcher(titles_keywords, \"TITLE\")\n",
    "\n",
    "    skills_found = set()\n",
    "    titles_found = set()\n",
    "\n",
    "    # PhraseMatcher results\n",
    "    for match_id, start, end in skills_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        skills_found.add(span.text)\n",
    "\n",
    "    for match_id, start, end in titles_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        titles_found.add(span.text)\n",
    "\n",
    "    # NER results\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_.lower() in {\"skill\", \"job_title\", \"work_of_art\", \"org\", \"product\"}:\n",
    "            # Add additional logic if needed to separate skills from titles\n",
    "            if \"developer\" in ent.text.lower() or \"engineer\" in ent.text.lower():\n",
    "                titles_found.add(ent.text)\n",
    "            else:\n",
    "                skills_found.add(ent.text)\n",
    "\n",
    "    return list(skills_found), list(titles_found)\n",
    "\n",
    "def process_resume(file_path, skills_folder, titles_folder):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Loading skills and job titles...\")\n",
    "    skills = load_keywords_from_folder(skills_folder)\n",
    "    titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "    print(\"[INFO] Extracting skills and titles using spaCy...\")\n",
    "    found_skills, found_titles = extract_keywords_with_spacy(text, skills, titles)\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(\"Skills Found:\", found_skills)\n",
    "    print(\"Job Titles Found:\", found_titles)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": found_skills,\n",
    "        \"titles\": found_titles\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\Users\\velam\\OneDrive\\Documents\\pavan's resume.docx\"\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\skills\"\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\titles\"\n",
    "    file_path2 = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\Imgdownload2.pdf\"\n",
    "    \n",
    "    result1 = process_resume(file_path, skills_folder, titles_folder)\n",
    "    result2 = process_resume(file_path2, skills_folder, titles_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64f099",
   "metadata": {},
   "source": [
    "lets do the same but with other dataset (given one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0891f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading DOCX file...\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " Velamala Pavan Krishna\n",
      "\n",
      "SUMMARY\n",
      "Passionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\n",
      "\n",
      "Phone:\n",
      "+91 7569637875\n",
      "Email:\n",
      "velamalapavankrishna@gmail.com\n",
      "Address: Visakhapatnam , \n",
      "Andhra Pradesh , India\n",
      "    530046\n",
      "\n",
      "Profile links\n",
      "\n",
      "Linkedin: linkedin.co \n",
      "\n",
      "Skills Found: ['management', '2024', 'emerging', 'India', 'REDUCTION', 'Passionate', 'Skills\\nTechnical', 'B', 'CONFERENCE', 'GPA', 'to', 'solutions', 'java', 'making', 'Certified', 'of', 'Interact', 'py', 'management system', 'Landmark', 'Thinking', 'data analysis', 'Large Language Models', 'Language', 'Development', 'Hack', 'DeepLearning', 'MySQL', 'Models', 'using', 'Problem-Solving      Team Collaboration \\n Communication       Critical Thinking\\nFlexible Adaptive', 'Google', 'computer', 'DNS', 'health', 'Web', 'Computer', 'Mental', 'Operating', 'scratch', 'Notebook', 'Web Developers', 'serving', 'skills', 'Library management', 'css', 'Soft', 'Member', 'with', 'html', 'HealthCare AI', 'Collaboration', 'impact', 'Bus', 'Java', 'links', 'Language Models', 'Team Collaboration', 'Using Python to Interact', 'creative', 'Flexible', 'Data', 'real', 'science', '7', 'OOPS', 'Python', 'Working', 'swing', 'Mental health', 'Azure', 'reservation system', 'AI Club', 'Skills', '9', 'Artificial Intelligence', 'world', '2023', 'continuous', 'detection', 'Eager', 'machine learning', 'IBM', 'Science', 'food serving', 'ML', 'Database', 'Email', 'HTML', 'Computer Science', 'Git', 'Algorithms', 'meaningful impact', 'ongoing', 'based', 'project', 'Tech', 'PROJECTS', 'and', 'ACTIVITIES', 'building', 'Linkedin', 'Hopkins', 'Structures', 'Library management system', 'System', 'Club', 'RISK REDUCTION', 'symptoms', 'RISK', 'John', 'Developers', 'CS', 'CSS', 'apply', 'student', 'Team', 'Communication', 'Diabetes', 'Proxy', 'Technical', 'Adaptive learning', 'computer science', 'AI', 'July', 'continuous learning', 'Fundamentals', 'hackathon', 'Critical Thinking', 'Phone', 'PhysioNet Challenge', 'fundamental', 'creative solutions', 'Server', 'technologies', 'C', 'prediction', 'AI & Development', 'collaboration', 'strong foundation', 'Soft skills:', 'emerging technologies', 'Matlab', 'model building', 'Microsoft', 'Soft skills', 'Github', 'team', 'a', 'Microsoft Certified', 'SUMMARY', 'by', 'RESILIENCE', 'Core', 'Iris', 'Tools', 'Web Development', 'skills:', 'Challenge', 'AI Fundamentals', 'learning', 'Profile', 'model', 'VScode', 'Solving', 'current', 'Technical skills', 'Javascript', 'Operating System', 'EDUCATION', 'Large', 'problems', 'Data Structures and Algorithms', 'Programming', 'Critical', 'Flask', 'ICTR-3 (INTERNATIONAL CONFERENCE ON TSUNAMI RISK REDUCTION& RESILIENCE', 'John Hopkins University', 'INTERNATIONAL', 'Python for Data Science', 'B-Tech', 'system', 'Data Science', 'strong', 'meaningful', 'assembler', 'logistic', 'University', 'reservation', 'data', 'machine', 'PyTorch', 'for', 'programming', 'PyTorch\\nMental', 'food', 'Tools:', 'at', 'Seminar', 'Dec', 'Generative AI', 'game', 'name', 'Technical skills:', 'Committed to continuous learning', 'Mar', 'Generative', 'Adaptive', 'Intelligence', 'chatbot', 'Problem', 'javascript', 'Jupyter Notebook', 'Azure AI', 'on', 'analysis', 'Address', 'foundation', 'HealthCare', 'Using', 'Library', 'Chess', 'socket', 'Data Structures', 'explore', 'Jupyter', 'Andhra Pradesh', 'EXPERIENCE', 'Committed', 'ON', 'CERTIFICATIONS', 'in', 'from']\n",
      "Job Titles Found: ['creative', 'management', 'Technical', 'Data', 'science', 'computer science', 'Python', 'B', '.', 'Azure', 'system', 'Data Science', 'java', 'assembler', 'logistic', 'Artificial Intelligence', 'Server', 'data analysis', 'data', 'Development', 'C', 'machine learning', 'Science', 'Models', 'Computer Science', 'health', 'Tech', 'javascript', 'System', 'Web Developers', 'HealthCare', 'RISK', 'Web Development', 'learning', 'in', 'model', 'Javascript', 'Java', 'student', 'EDUCATION']\n",
      "[INFO] Converting PDF pages to images...\n",
      "[INFO] Running OCR on page 1\n",
      "[INFO] Running OCR on page 2\n",
      "[INFO] Running OCR on page 3\n",
      "[INFO] Running OCR on page 4\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " DIGVIJAYSINGHPARMAR\n",
      "\n",
      "Permanent Address: 40/C, Sangam Nagar, Indore, M.P â€” 452006\n",
      "Present Address:2195 NiralaKunj, Gas AthourityOf India Township\n",
      "Dibiyapur, Auraiya, Uttar Pradesh, |ndia-206244\n",
      "\n",
      "Emait:digv10sp@gmail.com, digvijay10sp@outlook.com\n",
      "Mobile: +91-9662489663\n",
      "\n",
      "in ee Jinkedin.com/in/digvijay-singh-parmar-aabb0953\n",
      "\n",
      "CAREER OBJECTIVE\n",
      "\n",
      "To utilize my ability and knowledge about Safety and Emergency control methods, including\n",
      "the detection of risks to prevent life-threatening harms and damages  \n",
      "\n",
      "Skills Found: ['Oil and Gas Industry', 'for all', '2 years', 'improvements', 'Automatic', 'Languages', 'engineering', 'EMS', 'good', 'station', 'basis', 'Safety training', 'books', 'Fire Technology', '24', 'Incharge', 'make', 'work', 'Assuring', 'Training', 'Material', 'implementation', 'motivated', 'Participate in risk assessment', 'ready', 'SAFETY COUNCIL', 'PVT', 'INDUSTRIAL', 'with', 'Present', 'Safety', 'CAREER OBJECTIVE', 'Foam System', 'clean agent system', 'fire safety inspection', 'provide training', 'stipulations', 'OFFICER', 'Process Safety', 'NATIONAL SAFETY COUNCIL', 'national', 'Government', 'Work', 'Oil', 'Fire and Safety', 'power', 'conducting', 'water pump', 'Review', 'resources', 'Act', 'DEFENCE', 'QRA', 'Material master', 'risk', 'storage', 'PROFILE', 'audits', 'Certificate of proficiency', 'ISO 9001', 'other', 'incident', 'measure', 'PURSUITS', 'Identifying', 'INDUSTRIES', 'qualitative', 'reports', 'and', 'action', 'is', 'PERSONAL PROFILE', 'contractors', 'System', 'design and development', 'master control', 'protection', 'PSM', 'NATIONAL\\nCIVIL DEFENCE COLLAGE', 'Factories', 'listening', 'undertaking', 'Date of Birth', 'Safety Audits', 'as', 'safety inspection', 'hazardous', 'module', 'professional', 'applicable', 'emergency vehicles', 'preparation', 'safety training', 'department', 'people', 'specific job', 'safe', 'testing and operations', 'hotels', 'Rendering', 'Permanent', 'well', 'C', 'Marital Status: Married\\n\\nLanguages :', 'providing', 'CAREER', 'necessary', 'clean', 'SAFETY', 'IES', 'fire protection systems', 'training schedule', 'Uttar Pradesh', 'capable', 'Present Address', 'Fire and Safety audits', 'a', 'preliminary investigation', 'GAS', 'Governments', 'risks', 'fire protection', 'Internal Auditor', 'Emergency', '40', 'Responsible', 'projects', 'practices', 'hazardous waste', 'Skype', 'up', 'years', 'corrective', 'Health Safety at Work', 'technical expertise', 'self', 'Running', 'can', 'jobs', 'command', 'protection systems', 'activities', 'B.E) Fire Technology', 'Smoke & Heat Detection\\nSystem', 'easily', 'growth', 'working', 'planning and allocation', 'Loss', 'Reference', 'Process', 'PERSONALITY', 'music', 'Fire Safety Inspections & Audits/ Risk\\nAssessments', 'processes', 'inventory', 'responsibility', 'Purchase Requisitions', 'Carry', 'Valve', 'routine', 'always', 'globally', 'reliability', 'completion', 'associated risks', 'prevention', 'Heat', 'release', 'x', 'program', 'Birth', 'operations and maintenance', 'DCP', 'updated', 'operating', 'India', 'equipment', 'Plant', 'Inspections', 'OPITO', 'SOP preparation', 'other shift', 'proficiency', 'Safety IES IPS ACADEMY', 'Medium', 'National Level', 'specification', 'No', 'plant level', 'training', 'control', 'VERITAS', 'regular', 'standard', 'authorization', 'inventory management', 'From', 'Rie', 'good engineering practices', 'new', 'specific requirements', 'SENIOR', 'emergency response exercises', 'Date', 'personnel', 'IPS', 'protection system', 'Assessments', 'BUREAU', 'Inventory management', 'DISASTER MANAGEMENT', '1', 'e', 'damages', 'OF', 'mix', 'standard operating procedures', 'crewmembers', 'Status', 'BOCW', 'Experience', 'ine', 'Working', 'bites', 'SAP-PM & SAP-MM(such', 'Marital Status', 'Sit', 'Safety audits', 'requirements', 'planning', 'Smoke', 'Am', 'PM', 'reading books', 'Semi', 'Code', 'Excellent', 'apparatus', 'maintenance reports', 'declare', 'buildings', 'utilize', 'MANAGER', 'Participate', 'TECHNOLOGY', 'HSE', 'operating procedures', 'safety standards', 'level', 'SAP-PM & SAP-MM\\n\\n', 'material', 'Carryout', 'fire safety standards', 'Test', 'Dahej-Manufacturing Division', 'ISO', 'CO2 Flooding', 'oe', 'SENIOR OFFICER', 'Ensure', 'maintenance', 'development', 'et', 'ACADEMIC PURSUITS', 'SOP', 'CO2', 'PERSONAL', 'shop', 'SAP', 'list', 'life', 'Physical Test', 'technique', 'Detection', 'vehicles', 'emergency response', 'Hydrant System', 'Conducting', 'Gas', 'Risk', 'WORKING', 'investigation of incidents', 'organizing', 'hard working', 'Water', 'contro', 'NATIONAL SAFETY', 'ability', 'provides', 'provide', 'testing and maintenance', 'Provide', 'Foam', 'high', 'resource planning and allocation', 'international requirements', 'IMS Internal Auditor Certificate', 'additional', 'procedures.', 'Fire Safety', 'breathing apparatus', 'Ensure compliance', 'Evaluate', 'communication equipment', 'availability', 'Seminar', 'disposal', 'Spray', 'control system', 'NBC', 'FIRE TECHNOLOGY', 'High', 'Address', 'mobilization', 'Purchase Requisitions & Material', 'statute', 'executed', 'Process Safety Management (PSM)', 'dynamic', 'i', 'observation', 'friends', 'power plants', 'prevent', 'shift activities', 'waste', 'from', 'RA', 'management', 'adequacy', 'Achieving', 'high rise buildings', 'log', 'methods', 'accept', 'H', 'COUNCIL', 'to', 'WORKING EXPERIENCE', 'MM', 'properties', 'record', 'PVT LTD', 'report', 'of', 'operations', 'per', 'Physical', 'correctness', 'mitigation', 'knowledge', 'SCHOOL', 'ee', 'Gaseous', 'investigation', 'preparedness', 'round', 'Successful', 'Auditor', 'Heat Detection', 'Purchase', 'specific', 'engineering practices', 'DECLARATION', 'critical', 'industries', 'others', 'daily', 'NEBOSH', 'internal', 'Gaseous Flooding System', 'exercises', 'action points', 'within', 'correct', 'extinguishers', 'INDIA', 'Preparing', 'its', 'Up', 'Hindi\\n\\nPermanent Address', 'shift', 'internal audit', 'Reliance', '7', 'days', 'petrochemical Plant', 'NiralaKunj', 'fire drills', 'hard', 'book', 'performance', 'communication', 'ensure', 'including', 'reading', 'statutes', 'conditions', 'fire protection system', 'risk assessment (RA)', 'Organize', 'Flooding', 'based', 'required', 'Process Safety Management', 'INDUSTRIAL SAFETY', 'id', 'NBC Code', 'good growth', 'methodologies', 'emergency calls', 'resource', '14001', 'Swimming/Running/Chin-Up/Sit-Up/Push-Up', 'transportation', 'International', 'points', 'all', 'assist', 'To', 'documents', 'challenges', 'aspects', 'plant', 'schedule', 'audit', 'standard operating', 'ACADEMY', 'inspection', 'OHSAS', 'required resources', 'Health', 'ISO 14001 & 18001', 'site', 'Technology .', 'VIJAYNAGAR INDORE Lette\\nPERSONALITY TRAIT', 'personality', 'IES IPS ACADEMY', 'compliance', 'Suggest', 'OBJECTIVE', 'foam', 'enhancement', 'set', 'Oil and Gas industries', 'up to', 'trainings', 'use', 'guidance', 'water', 'firefighting equipment', 'turnaround', 'testing', 'house', 'SUMMARY', 'modules', 'am', 'Position', 'resource planning', 'ai', 'information', 'Township', 'steps', 'environment', 'Safe', 'worker', 'believe', 'flooding', 'response', 'English', 'incidents', 'documentations', 'Safety and Emergency', 'high rise', '2', 'Inventory', 'Certificate', 'job', 'OISD', 'maintaining', 'Manufacturing', 'Gas Industry', 'expertise', 'client specification', 'Permanent Address', 'responsibilities', 'store', 'on site', 'Develop', 'Mobile', 'measures', 'PT', 'award', 'Maintain', 'Hindi', 'rise', 'checklist', 'Division', 'reservation', 'OFFICER - FIRE & SAFETY', 'agent', 'Total', 'versed', 'mitigate', 'Fire Safety Audits', 'are', 'executive', 'proficiency in', 'LTD', 'the set', 'Develop/Evaluate', 'accidents', 'fire safety', 'any', 'clean agent', 'National', 'Fire water', 'on', 'Safety professional', 'strive', 'in design', 'OH&S', 'design', 'NAGPUR', 'EXPERIENCE', 'Push', 'training modules', 'CERTIFICATIONS', 'Designing', 'insure', 'emergency', 'procedures', 'Name', 'incident analysis', 'Industry', 'interest', 'providing recommendations', 'Technology', 'smile', 'organization', 'calls', 'jeeps', 'Undergone 2D+3D Designing', 'sheet', 'confident', 'Safety Management', 'FIRE & SAFETY', '2016', 'MNC', 'Minimum', 'Fire Hydrant System', 'reference', 'INDORE', 'breathing', 'Successful completion', 'Codes', 'certification', 'always strive', 'Mig', 'Bachelor', 'Fire protection', 'Engineering', 'prospect', 'Requisitions', 'Fixed', 'ISO 14001', 'coordination', 'course', 'risk assessment', 'Swimming', 'Internal', 'Fire', 'Immediate response', 'Level', 'Management', 'Fire protection systems', 'communications', 'ISO-14001', 'face', 'Second', 'HSE training', 'log book', 'Assure', 'detection', 'Fire & Safety', 'also', 'Safety at Work', 'pear', 'allocation', 'IMS', 'advice', 'Oil and Gas', 'Reliance Physical Test Examination', 'assessment', 'Maintaining', 'High Velocity', 'working environment', 'petrochemical', 'systems', 'Velocity Water Spray System', 'incident command', 'driven', 'material availability', 'Hobbies', 'MANAGEMENT', 'CERTIFICATIONS& EXTRA- CURICULLAR ACTIVITES', 'RELIANCE', 'Audits', 'out', 'CIVIL', 'GOVT', 'pump', 'de', 'mee', 'Examination', 'portable', 'July', 'Health Safety', 'firefighting', 'NATIONAL', 'Semi Fixed Foam System', 'ACADEMIC', 'conduct', 'ENGINEERING', 'Preparation', 'employees', 'client', 'fire extinguishers', 'Passport', '9001', 'Safety & Loss', 'NFPA', 'drills', 'COLLAGE', 'Carry out', 'if', 'gain', 'permit', 'maintenance work', 'fire', 'standards', 'Extinguisher', 'NFPA\\nCodes', 'RELIANCE INDUSTRIES LIMITED', 'due', 'OHSAS-18001', 'recommend', 'Safety Inspections', 'Assist', 'IS', 'French', 'relating', 'travelling', 'in life', 'international', 'new projects', 'safe storage', 'upcoming projects', 'mock drills', 'LIMITED', 'additional responsibilities', 'control methods', 'Safety Training', 'master', 'SAFETY ENGINEERING', 'system', 'recommendations', '18001', 'Exhibit', 'safety', 'cooking', 'FIRE', 'HST', 'University', 'check', 'global', 'for', 'training department', 'request', 'operability', 'at', 'Yearly', 'Languages :', 'DISASTER', 'sessions', 'Fire Safety Inspections', 'Deluge', 'd', 'analysis', 'Velocity', 'modifications', 'Manual', 'related', 'Achieving Excellent Position', 'international standards', 'Firefighting', 'technical', 'permit request', '2009', 'plants', 'in', 'smoke']\n",
      "Job Titles Found: ['management', 'Manufacturing', 'equipment', 'OFFICER', 'audit', 'SENIOR OFFICER', 'internal audit', 'FIRE TECHNOLOGY & SAFETY ENGINEERING', 'as', 'Management', 'inspection', 'maintenance', 'Health', '$', 'Medium', 'development', 'engineering', '.', 'master', 'Process', 'Technology', 'system', 'communications', 'PT', 'training', 'ACADEMIC', 'music', 'operations', 'requirements', 'SAP', 'compliance', 'inventory', 'agent', 'HSE training', 'ENGINEERING', 'C', 'Fire Safety', 'risk', 'Am', 'PM', 'Training', 'SENIOR', 'Auditor', 'executive', 'personnel', 'Test', 'IPS', 'other', 'Risk', 'fire safety', '1', 'am', 'Engineering', 'Position', 'x', 'System', 'MANAGER', 'INDUSTRIAL', 'Safety professional', 'TECHNOLOGY', 'environment', 'Responsible', 'e', 'HSE', 'Internal Auditor', 'systems', 'design', 'transportation', 'technical', 'Carryout', '2', 'Inventory', 'job', 'MANAGEMENT', 'in', 'CIVIL']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "\n",
    "# Set your poppler and tesseract paths\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    kw_clean = line.strip().lower()\n",
    "                    if kw_clean:\n",
    "                        keywords.add(kw_clean)\n",
    "    return list(keywords)\n",
    "\n",
    "def create_phrase_matcher(keywords, label):\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = [nlp.make_doc(text) for text in keywords]\n",
    "    matcher.add(label, patterns)\n",
    "    return matcher\n",
    "\n",
    "def extract_keywords_with_spacy(text, skills_keywords, titles_keywords):\n",
    "    doc = nlp(text)\n",
    "    skills_matcher = create_phrase_matcher(skills_keywords, \"SKILL\")\n",
    "    titles_matcher = create_phrase_matcher(titles_keywords, \"TITLE\")\n",
    "\n",
    "    skills_found = set()\n",
    "    titles_found = set()\n",
    "\n",
    "    # PhraseMatcher results\n",
    "    for match_id, start, end in skills_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        skills_found.add(span.text)\n",
    "\n",
    "    for match_id, start, end in titles_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        titles_found.add(span.text)\n",
    "\n",
    "    # NER results\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_.lower() in {\"skill\", \"job_title\", \"work_of_art\", \"org\", \"product\"}:\n",
    "            # Add additional logic if needed to separate skills from titles\n",
    "            if \"developer\" in ent.text.lower() or \"engineer\" in ent.text.lower():\n",
    "                titles_found.add(ent.text)\n",
    "            else:\n",
    "                skills_found.add(ent.text)\n",
    "\n",
    "    return list(skills_found), list(titles_found)\n",
    "\n",
    "def process_resume(file_path, skills_folder, titles_folder):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Loading skills and job titles...\")\n",
    "    skills = load_keywords_from_folder(skills_folder)\n",
    "    titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "    print(\"[INFO] Extracting skills and titles using spaCy...\")\n",
    "    found_skills, found_titles = extract_keywords_with_spacy(text, skills, titles)\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(\"Skills Found:\", found_skills)\n",
    "    print(\"Job Titles Found:\", found_titles)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": found_skills,\n",
    "        \"titles\": found_titles\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\Users\\velam\\OneDrive\\Documents\\pavan's resume.docx\"\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_skills_parts\"\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_titles_parts\"\n",
    "    file_path2 = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\Imgdownload2.pdf\"\n",
    "    \n",
    "    result1 = process_resume(file_path, skills_folder, titles_folder)\n",
    "    result2 = process_resume(file_path2, skills_folder, titles_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27eb36",
   "metadata": {},
   "source": [
    "new gpt trail code (leave it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0bf6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading DOCX file...\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " Velamala Pavan Krishna\n",
      "\n",
      "SUMMARY\n",
      "Passionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\n",
      "\n",
      "Phone:\n",
      "+91 7569637875\n",
      "Email:\n",
      "velamalapavankrishna@gmail.com\n",
      "Address: Visakhapatnam , \n",
      "Andhra Pradesh , India\n",
      "    530046\n",
      "\n",
      "Profile links\n",
      "\n",
      "Linkedin: linkedin.co \n",
      "\n",
      "Skills Found: ['AI', 'AI & Development', 'AI Club', 'AI Fundamentals', 'Adaptive learning', 'Algorithms', 'Andhra Pradesh', 'Artificial Intelligence', 'Azure', 'B-Tech', 'Bus', 'CERTIFICATIONS', 'CS', 'CSS', 'Chess', 'Collaboration', 'Computer Science', 'Critical Thinking', 'Data Science', 'Data Structures', 'Database', 'DeepLearning', 'Developers', 'Diabetes', 'Email', 'Flask', 'Fundamentals', 'GPA', 'Git', 'Github', 'Google', 'HTML', 'HealthCare', 'IBM', 'Intelligence', 'Iris', 'Java', 'Javascript', 'John Hopkins University', 'Library management', 'Linkedin', 'ML', 'Mar', 'Matlab', 'Mental health', 'Microsoft', 'Models', 'MySQL', 'PhysioNet Challenge', 'Proxy', 'PyTorch\\nMental', 'Python', 'Python for Data Science', 'RISK', 'RISK REDUCTION', 'Skills\\nTechnical', 'Soft skills', 'Using Python to Interact', 'VScode', 'Web', 'Web Development', 'assembler', 'building', 'collaboration', 'computer science', 'css', 'data analysis', 'emerging technologies', 'foundation', 'health', 'html', 'java', 'javascript', 'learning', 'machine learning', 'management system', 'model building', 'prediction', 'project', 'py', 'scratch']\n",
      "Job Titles Found: []\n",
      "[INFO] Converting PDF pages to images...\n",
      "[INFO] Running OCR on page 1\n",
      "[INFO] Running OCR on page 2\n",
      "[INFO] Running OCR on page 3\n",
      "[INFO] Running OCR on page 4\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using spaCy...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " DIGVIJAYSINGHPARMAR\n",
      "\n",
      "Permanent Address: 40/C, Sangam Nagar, Indore, M.P â€” 452006\n",
      "Present Address:2195 NiralaKunj, Gas AthourityOf India Township\n",
      "Dibiyapur, Auraiya, Uttar Pradesh, |ndia-206244\n",
      "\n",
      "Emait:digv10sp@gmail.com, digvijay10sp@outlook.com\n",
      "Mobile: +91-9662489663\n",
      "\n",
      "in ee Jinkedin.com/in/digvijay-singh-parmar-aabb0953\n",
      "\n",
      "CAREER OBJECTIVE\n",
      "\n",
      "To utilize my ability and knowledge about Safety and Emergency control methods, including\n",
      "the detection of risks to prevent life-threatening harms and damages  \n",
      "\n",
      "Skills Found: ['Achieving Excellent Position', 'B.E) Fire Technology', 'BOCW', 'Birth', 'CO2', 'CO2 Flooding', 'COLLAGE', 'Carryout', 'Codes', 'Conducting', 'DCP', 'Dahej-Manufacturing Division', 'Develop/Evaluate', 'EMS', 'Fire & Safety', 'Fire Hydrant System', 'Fire Safety', 'Fire Safety Audits', 'Fire and Safety', 'Fire protection', 'Firefighting', 'Foam', 'French', 'GAS', 'GOVT', 'Gas', 'Gas Industry', 'Gaseous Flooding System', 'Government', 'HSE', 'HST', 'Health', 'Heat', 'Hindi', 'Hindi\\n\\nPermanent Address', 'Hobbies', 'Hydrant System', 'IES IPS ACADEMY', 'IMS', 'IMS Internal Auditor Certificate', 'INDORE', 'INDUSTRIAL SAFETY', 'IPS', 'ISO', 'ISO 14001', 'ISO 14001 & 18001', 'ISO 9001', 'ISO-14001', 'LTD', 'Languages', 'Maintain', 'Manufacturing', 'Marital Status: Married\\n\\nLanguages :', 'Material', 'Mig', 'Mobile', 'NAGPUR', 'NATIONAL\\nCIVIL DEFENCE COLLAGE', 'NATIONAL SAFETY COUNCIL', 'NBC Code', 'NEBOSH', 'NFPA', 'NFPA\\nCodes', 'NiralaKunj', 'OFFICER - FIRE & SAFETY', 'OH&S', 'OHSAS-18001', 'OISD', 'Oil', 'Oil and Gas', 'PSM', 'PVT LTD', 'Participate', 'Passport', 'Preparation', 'Present Address', 'Process Safety', 'Process Safety Management', 'Purchase Requisitions', 'Purchase Requisitions & Material', 'QRA', 'RA', 'RELIANCE INDUSTRIES LIMITED', 'Reliance Physical Test Examination', 'Rendering', 'Requisitions', 'Rie', 'Risk', 'Running', 'SAFETY ENGINEERING', 'SAP', 'SAP-PM & SAP-MM\\n\\n', 'SAP-PM & SAP-MM(such', 'SCHOOL', 'Safe', 'Safety & Loss', 'Safety IES IPS ACADEMY', 'Safety Training', 'Safety training', 'Semi Fixed Foam System', 'Sit', 'Skype', 'Smoke', 'Smoke & Heat Detection\\nSystem', 'Spray', 'Status', 'Suggest', 'Swimming', 'Swimming/Running/Chin-Up/Sit-Up/Push-Up', 'TECHNOLOGY', 'Technology', 'Training', 'Undergone 2D+3D Designing', 'Uttar Pradesh', 'VERITAS', 'Velocity', 'Velocity Water Spray System', 'Water', 'assessment', 'authorization', 'award', 'buildings', 'command', 'completion', 'conducting', 'contractors', 'contro', 'cooking', 'design', 'disposal', 'ee', 'fire protection', 'fire safety', 'firefighting', 'foam', 'high rise', 'incident command', 'internal audit', 'international standards', 'investigation', 'maintenance', 'master control', 'measures', 'mitigation', 'mobilization', 'mock', 'modifications', 'observation', 'oe', 'on site', 'operations', 'organization', 'pear', 'petrochemical', 'plants', 'power plants', 'preparation', 'preparedness', 'prevention', 'prospect', 'protection', 'protection systems', 'record', 'risk', 'risk assessment', 'safety training', 'smoke', 'statutes', 'steps', 'storage', 'store', 'testing', 'training', 'transportation', 'waste', 'water']\n",
      "Job Titles Found: ['FIRE TECHNOLOGY & SAFETY ENGINEERING']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "\n",
    "# Set your poppler and tesseract paths\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Blocklists for filtering false positives\n",
    "SKILL_BLOCKLIST = {\"communication\", \"teamwork\", \"microsoft office\", \"ability\", \"work\", \"english\"}\n",
    "TITLE_BLOCKLIST = {\"member\", \"participant\", \"student\"}\n",
    "\n",
    "# Context triggers for skills\n",
    "SKILL_CONTEXT_TRIGGERS = [\"experienced\", \"proficient\", \"skilled\", \"knowledge\", \"familiar\"]\n",
    "SKILL_PREPOSITIONS = [\"in\", \"with\", \"on\"]\n",
    "\n",
    "# Common job title keywords\n",
    "TITLE_KEYWORDS = [\"developer\", \"engineer\", \"manager\", \"specialist\", \"consultant\", \"analyst\"]\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    kw_clean = line.strip().lower()\n",
    "                    if kw_clean:\n",
    "                        keywords.add(kw_clean)\n",
    "    return list(keywords)\n",
    "\n",
    "def create_phrase_matcher(keywords, label):\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = [nlp.make_doc(text) for text in keywords]\n",
    "    matcher.add(label, patterns)\n",
    "    return matcher\n",
    "\n",
    "def is_valid_skill(text):\n",
    "    text_lower = text.lower()\n",
    "    if any(blocked in text_lower for blocked in SKILL_BLOCKLIST):\n",
    "        return False\n",
    "    if len(text) < 2 or len(text) > 40:\n",
    "        return False\n",
    "    # Optionally allow digits for versions (e.g., Python 3)\n",
    "    # if any(char.isdigit() for char in text):\n",
    "    #     return False\n",
    "    return True\n",
    "\n",
    "def is_valid_title(text):\n",
    "    text_lower = text.lower()\n",
    "    if any(blocked in text_lower for blocked in TITLE_BLOCKLIST):\n",
    "        return False\n",
    "    if not any(keyword in text_lower for keyword in TITLE_KEYWORDS):\n",
    "        return False\n",
    "    if len(text) < 2 or len(text) > 60:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def find_contextual_skills(doc):\n",
    "    skills = set()\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.lemma_.lower() in SKILL_CONTEXT_TRIGGERS:\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"prep\" and child.text.lower() in SKILL_PREPOSITIONS:\n",
    "                        for obj in child.children:\n",
    "                            if obj.dep_ == \"pobj\" and is_valid_skill(obj.text):\n",
    "                                skills.add(obj.text)\n",
    "    return skills\n",
    "\n",
    "def extract_keywords_with_spacy(text, skills_keywords, titles_keywords):\n",
    "    doc = nlp(text)\n",
    "    skills_matcher = create_phrase_matcher(skills_keywords, \"SKILL\")\n",
    "    titles_matcher = create_phrase_matcher(titles_keywords, \"TITLE\")\n",
    "\n",
    "    skills_found = set()\n",
    "    titles_found = set()\n",
    "\n",
    "    # PhraseMatcher results\n",
    "    for match_id, start, end in skills_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        if is_valid_skill(span.text):\n",
    "            skills_found.add(span.text)\n",
    "\n",
    "    for match_id, start, end in titles_matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        if is_valid_title(span.text):\n",
    "            titles_found.add(span.text)\n",
    "\n",
    "    # Contextual skill extraction\n",
    "    skills_found.update(find_contextual_skills(doc))\n",
    "\n",
    "    # NER results\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_.lower() in {\"skill\", \"job_title\", \"work_of_art\", \"org\", \"product\"}:\n",
    "            if any(keyword in ent.text.lower() for keyword in TITLE_KEYWORDS):\n",
    "                if is_valid_title(ent.text):\n",
    "                    titles_found.add(ent.text)\n",
    "            else:\n",
    "                if is_valid_skill(ent.text):\n",
    "                    skills_found.add(ent.text)\n",
    "\n",
    "    return sorted(skills_found), sorted(titles_found)\n",
    "\n",
    "def process_resume(file_path, skills_folder, titles_folder):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Loading skills and job titles...\")\n",
    "    skills = load_keywords_from_folder(skills_folder)\n",
    "    titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "    print(\"[INFO] Extracting skills and titles using spaCy...\")\n",
    "    found_skills, found_titles = extract_keywords_with_spacy(text, skills, titles)\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(\"Skills Found:\", found_skills)\n",
    "    print(\"Job Titles Found:\", found_titles)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": found_skills,\n",
    "        \"titles\": found_titles\n",
    "    }\n",
    "\n",
    "# === EXAMPLE USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\Users\\velam\\OneDrive\\Documents\\pavan's resume.docx\"\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\skills\"\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\titles\"\n",
    "    file_path2 = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\Imgdownload2.pdf\"\n",
    "    \n",
    "    result1 = process_resume(file_path, skills_folder, titles_folder)\n",
    "    result2 = process_resume(file_path2, skills_folder, titles_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662ae9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\velam\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download and load the model from Hugging Face\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", token=False)\n",
    "\n",
    "\n",
    "# Save the model to your desired local path\n",
    "model.save(r\"C:\\amrita_uni\\Projects\\asmacs internship\\model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b42fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "sentence-transformers/all-MiniLM-L6-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-683fff0c-2dd2ac216386de176a058472;68938064-fa2d-4792-91b4-51981de34c65)\n\nRepository Not Found for url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid credentials in Authorization header",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m pytesseract\u001b[38;5;241m.\u001b[39mpytesseract\u001b[38;5;241m.\u001b[39mtesseract_cmd \u001b[38;5;241m=\u001b[39m tesseract_cmd\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load Sentence Transformer model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_pdf\u001b[39m(pdf_path):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Converting PDF pages to images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:321\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    309\u001b[0m         modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[0;32m    310\u001b[0m             model_name_or_path,\n\u001b[0;32m    311\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    318\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    319\u001b[0m         )\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[0;32m    334\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:1606\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m tokenizer_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs}\n\u001b[0;32m   1604\u001b[0m config_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs}\n\u001b[1;32m-> 1606\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1614\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\models\\Transformer.py:80\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 80\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\models\\Transformer.py:121\u001b[0m, in \u001b[0;36mTransformer._load_config\u001b[1;34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_config\u001b[39m(\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m, model_name_or_path: \u001b[38;5;28mstr\u001b[39m, cache_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, backend: \u001b[38;5;28mstr\u001b[39m, config_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[0;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[PeftConfig \u001b[38;5;241m|\u001b[39m PretrainedConfig, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads the transformers or PEFT configuration\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m        tuple[PretrainedConfig, bool]: The model configuration and a boolean indicating whether the model is a PEFT model.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 121\u001b[0m         \u001b[43mfind_adapter_config_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal_files_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     ):\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft_available():\n\u001b[0;32m    131\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a PEFT model requires installing the `peft` package. You can install it via `pip install peft`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m             )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\peft_utils.py:88\u001b[0m, in \u001b[0;36mfind_adapter_config_file\u001b[1;34m(model_id, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, _commit_hash)\u001b[0m\n\u001b[0;32m     86\u001b[0m         adapter_cached_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_id, ADAPTER_CONFIG_NAME)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     adapter_cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mADAPTER_CONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_cached_filename\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: sentence-transformers/all-MiniLM-L6-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Set your poppler and tesseract paths\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    kw_clean = line.strip().lower()\n",
    "                    if kw_clean:\n",
    "                        keywords.add(kw_clean)\n",
    "    return list(keywords)\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # Use inner product for cosine similarity (with normalized vectors)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def extract_keywords_with_faiss(text, skills_keywords, titles_keywords):\n",
    "    # Embed the input text as a single chunk\n",
    "    text_embedding = model.encode([text], normalize_embeddings=True)\n",
    "\n",
    "    # Embed keywords\n",
    "    skills_embeddings = model.encode(skills_keywords, normalize_embeddings=True)\n",
    "    titles_embeddings = model.encode(titles_keywords, normalize_embeddings=True)\n",
    "\n",
    "    # Create FAISS indices\n",
    "    skills_index = create_faiss_index(skills_embeddings)\n",
    "    titles_index = create_faiss_index(titles_embeddings)\n",
    "\n",
    "    # Retrieve top matches (top 10)\n",
    "    k = 10\n",
    "    D_skills, I_skills = skills_index.search(text_embedding, k)\n",
    "    D_titles, I_titles = titles_index.search(text_embedding, k)\n",
    "\n",
    "    # Collect matched keywords with similarity scores (threshold > 0.5)\n",
    "    matched_skills = []\n",
    "    for idx, i in enumerate(I_skills[0]):\n",
    "        sim = float(D_skills[0][idx])\n",
    "        if sim > 0.5:\n",
    "            matched_skills.append((skills_keywords[i], sim))\n",
    "\n",
    "    matched_titles = []\n",
    "    for idx, i in enumerate(I_titles[0]):\n",
    "        sim = float(D_titles[0][idx])\n",
    "        if sim > 0.5:\n",
    "            matched_titles.append((titles_keywords[i], sim))\n",
    "\n",
    "    # Remove duplicates (keep highest similarity)\n",
    "    skills_dict = {}\n",
    "    for skill, sim in matched_skills:\n",
    "        if skill not in skills_dict or sim > skills_dict[skill]:\n",
    "            skills_dict[skill] = sim\n",
    "    titles_dict = {}\n",
    "    for title, sim in matched_titles:\n",
    "        if title not in titles_dict or sim > titles_dict[title]:\n",
    "            titles_dict[title] = sim\n",
    "\n",
    "    # Sort by similarity descending\n",
    "    matched_skills = sorted(skills_dict.items(), key=lambda x: -x[1])\n",
    "    matched_titles = sorted(titles_dict.items(), key=lambda x: -x[1])\n",
    "\n",
    "    return matched_skills, matched_titles\n",
    "\n",
    "def process_resume(file_path, skills_folder, titles_folder):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Loading skills and job titles...\")\n",
    "    skills = load_keywords_from_folder(skills_folder)\n",
    "    titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "    print(\"[INFO] Extracting skills and titles using FAISS and Sentence Transformer...\")\n",
    "    found_skills, found_titles = extract_keywords_with_faiss(text, skills, titles)\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(\"Skills Found (with similarity):\")\n",
    "    for skill, sim in found_skills:\n",
    "        print(f\"{skill}: {sim:.4f}\")\n",
    "    print(\"Job Titles Found (with similarity):\")\n",
    "    for title, sim in found_titles:\n",
    "        print(f\"{title}: {sim:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": found_skills,\n",
    "        \"titles\": found_titles\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\Users\\velam\\OneDrive\\Documents\\pavan's resume.docx\"\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_skills_parts\"\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_titles_parts\"\n",
    "    file_path2 = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\Imgdownload2.pdf\"\n",
    "\n",
    "    result1 = process_resume(file_path, skills_folder, titles_folder)\n",
    "    result2 = process_resume(file_path2, skills_folder, titles_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6167f992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading DOCX file...\n",
      "[INFO] Loading skills and job titles...\n",
      "[INFO] Extracting skills and titles using FAISS and Sentence Transformer...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " Velamala Pavan Krishna\n",
      "\n",
      "SUMMARY\n",
      "Passionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\n",
      "\n",
      "Phone:\n",
      "+91 7569637875\n",
      "Email:\n",
      "velamalapavankrishna@gmail.com\n",
      "Address: Visakhapatnam , \n",
      "Andhra Pradesh , India\n",
      "    530046\n",
      "\n",
      "Profile links\n",
      "\n",
      "Linkedin: linkedin.co \n",
      "\n",
      "Skills Found (with confidence):\n",
      "ai health tech: 0.5893\n",
      "ai compute experience: 0.5852\n",
      "microsoft ai: 0.5743\n",
      "azure ai: 0.5704\n",
      "ai platform  (openai): 0.5597\n",
      "ai developer: 0.5567\n",
      "ai training tool: 0.5562\n",
      "ai integration developer: 0.5551\n",
      "ai software: 0.5546\n",
      "ai compute: 0.5529\n",
      "Job Titles Found (with confidence):\n",
      "machine learning & genai engineer: 0.5920\n",
      "machine learning engineer/ researcher - 2021 programme: 0.5831\n",
      "future opportunity: azure ai architect: 0.5799\n",
      "data scientist - optimus ai: 0.5790\n",
      "machine learning engineer 03-se1220-2: 0.5784\n",
      "data engineer&nbsp;with ai: 0.5715\n",
      "\"#sgunitedjobs scientist (ai for healthcare), computing &amp; intelligence, ihpc\": 0.5713\n",
      "data science specialist - optimus ai: 0.5710\n",
      "ml ai engineer: 0.5708\n",
      "ai solutions developer: 0.5679\n",
      "{'resumeData': {'basicDetails': {'name': {'value': 'Velamala', 'confidence': 9.0}, 'lastname': {'value': 'Krishna', 'confidence': 9.0}, 'email': {'value': 'velamalapavankrishna@gmail.com', 'confidence': 9.0}, 'phone': {'value': '+91 7569637875\\n', 'confidence': 8.5}, 'address': {'value': '', 'confidence': 0.0}}, 'workExperience': [], 'education': [], 'skills': [{'skill': {'value': 'ai health tech', 'confidence': 5.89}}, {'skill': {'value': 'ai compute experience', 'confidence': 5.85}}, {'skill': {'value': 'microsoft ai', 'confidence': 5.74}}, {'skill': {'value': 'azure ai', 'confidence': 5.7}}, {'skill': {'value': 'ai platform  (openai)', 'confidence': 5.6}}, {'skill': {'value': 'ai developer', 'confidence': 5.57}}, {'skill': {'value': 'ai training tool', 'confidence': 5.56}}, {'skill': {'value': 'ai integration developer', 'confidence': 5.55}}, {'skill': {'value': 'ai software', 'confidence': 5.55}}, {'skill': {'value': 'ai compute', 'confidence': 5.53}}], 'certifications': []}, 'summary': {'value': '', 'confidence': 0.0}, 'htmlContent': '', 'pdfText': 'Velamala Pavan Krishna\\n\\nSUMMARY\\nPassionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\\n\\nPhone:\\n+91 7569637875\\nEmail:\\nvelamalapavankrishna@gmail.com\\nAddress: Visakhapatnam , \\nAndhra Pradesh , India\\n    530046\\n\\nProfile links\\n\\nLinkedin: linkedin.com/in/pavan-krishna-velamala-6a92062b4\\nGithub: https://github.com/W4RG0Dpk\\nEDUCATION\\nB-Tech in Computer Science and Artificial Intelligence | 2023-2027\\nAmrita Vishwa Vidyapeetham, Amritapuri, Kollam, Kerala | GPA : 9.11\\n\\nEXPERIENCE\\nHealthCare AI Club Member | 2025 - current\\nAmrita Vishwa Vidyapeetham \\nWorking on George B. Moody PhysioNet Challenge (team name: HAI_AMRITIANS)\\n\\nCERTIFICATIONS\\nMicrosoft Certified: Azure AI Fundamentals by coincent (2023-2024)\\nPython for Data Science, AI & Development by IBM (July 23, 2024)\\nHTML, CSS, and Javascript for Web Developers by John Hopkins University (Aug 4, 2024)\\nGenerative AI with Large Language Models by DeepLearning.AI (Dec 7, 2024)\\nUsing Python to Interact with the Operating System by Google (Mar 9, 2025)\\n\\nPROJECTS\\nHack assembler in py (from scratch)\\nIris classifier (from scratch)\\nBus reservation system using html ,css, javascript\\nLibrary management system using swing , java\\nChess game-AI\\nCentralized-DNS-with-Proxy-DNS-Server-using-socket-programming\\nLandmark detection using PyTorch\\nMental health chatbot ( Epochon hackathon 2024 )\\nDiabetes prediction using logistic regresison (from scratch)\\nDiesease prediction based on symptoms (ongoing ML project)\\nChagas detection (George B. Moody PhysioNet Challenge) [ongoing]\\n\\nACTIVITIES\\nVolunteered at Amritavarsham 70 & 71.\\nVolunteered for food serving at ICTR-3 (INTERNATIONAL CONFERENCE ON TSUNAMI RISK REDUCTION& RESILIENCE) Seminar.\\n\\nSkills\\nTechnical skills:\\nProgramming : Python, Java, C \\nCore CS: Data Structures and Algorithms , OOPS\\nAI : ML fundamental ,  Fundementals of AI\\nWeb Development: Flask, HTML, CSS\\nDatabase: MySQL\\nTools: Jupyter Notebook, Matlab , VScode , Git\\nSoft skills:\\nProblem-Solving      Team Collaboration \\n Communication       Critical Thinking\\nFlexible Adaptive learning    '}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Set your poppler and tesseract paths\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "\n",
    "# Load spaCy small model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Sentence Transformer model from local path\n",
    "model = SentenceTransformer(r\"C:\\amrita_uni\\Projects\\asmacs internship\\model\")\n",
    "\n",
    "# Regex patterns\n",
    "email_pattern = r\"[\\w\\.-]+@[\\w\\.-]+\"\n",
    "phone_pattern = r\"\\+?\\d[\\d\\s\\-]{8,}\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\") or filename.endswith(\".csv\"):\n",
    "            with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    kw_clean = line.strip().lower()\n",
    "                    if kw_clean:\n",
    "                        keywords.add(kw_clean)\n",
    "    return list(keywords)\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def extract_keywords_with_faiss(text, skills_keywords, titles_keywords, k=10, threshold=0.5):\n",
    "    # Embed the input text as a single chunk\n",
    "    text_embedding = model.encode([text], normalize_embeddings=True)\n",
    "\n",
    "    # Embed keywords\n",
    "    skills_embeddings = model.encode(skills_keywords, normalize_embeddings=True)\n",
    "    titles_embeddings = model.encode(titles_keywords, normalize_embeddings=True)\n",
    "\n",
    "    # Create FAISS indices\n",
    "    skills_index = create_faiss_index(skills_embeddings)\n",
    "    titles_index = create_faiss_index(titles_embeddings)\n",
    "\n",
    "    # Retrieve top matches\n",
    "    D_skills, I_skills = skills_index.search(text_embedding, k)\n",
    "    D_titles, I_titles = titles_index.search(text_embedding, k)\n",
    "\n",
    "    # Collect matched keywords with similarity scores (threshold > 0.5)\n",
    "    matched_skills = []\n",
    "    for idx, i in enumerate(I_skills[0]):\n",
    "        sim = float(D_skills[0][idx])\n",
    "        if sim > threshold:\n",
    "            matched_skills.append((skills_keywords[i], sim))\n",
    "\n",
    "    matched_titles = []\n",
    "    for idx, i in enumerate(I_titles[0]):\n",
    "        sim = float(D_titles[0][idx])\n",
    "        if sim > threshold:\n",
    "            matched_titles.append((titles_keywords[i], sim))\n",
    "\n",
    "    # Remove duplicates (keep highest similarity)\n",
    "    skills_dict = {}\n",
    "    for skill, sim in matched_skills:\n",
    "        if skill not in skills_dict or sim > skills_dict[skill]:\n",
    "            skills_dict[skill] = sim\n",
    "    titles_dict = {}\n",
    "    for title, sim in matched_titles:\n",
    "        if title not in titles_dict or sim > titles_dict[title]:\n",
    "            titles_dict[title] = sim\n",
    "\n",
    "    # Sort by similarity descending\n",
    "    matched_skills = sorted(skills_dict.items(), key=lambda x: -x[1])\n",
    "    matched_titles = sorted(titles_dict.items(), key=lambda x: -x[1])\n",
    "\n",
    "    return matched_skills, matched_titles\n",
    "\n",
    "def extract_email(text):\n",
    "    match = re.search(email_pattern, text)\n",
    "    return (match.group(0), 9.0) if match else (\"\", 0.0)\n",
    "\n",
    "def extract_phone(text):\n",
    "    match = re.search(phone_pattern, text)\n",
    "    return (match.group(0), 8.5) if match else (\"\", 0.0)\n",
    "\n",
    "def extract_name(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            parts = ent.text.split()\n",
    "            if parts:\n",
    "                return (parts[0], 9.0)\n",
    "    return (\"\", 0.0)\n",
    "\n",
    "def extract_lastname(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            parts = ent.text.split()\n",
    "            if parts:\n",
    "                return (parts[-1], 9.0)\n",
    "    return (\"\", 0.0)\n",
    "\n",
    "def process_resume(file_path, skills_folder, titles_folder):\n",
    "    # Extract text from file\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Loading skills and job titles...\")\n",
    "    skills = load_keywords_from_folder(skills_folder)\n",
    "    titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "    print(\"[INFO] Extracting skills and titles using FAISS and Sentence Transformer...\")\n",
    "    found_skills, found_titles = extract_keywords_with_faiss(text, skills, titles)\n",
    "\n",
    "    email, email_conf = extract_email(text)\n",
    "    phone, phone_conf = extract_phone(text)\n",
    "    name, name_conf = extract_name(text)\n",
    "    lastname, lastname_conf = extract_lastname(text)\n",
    "\n",
    "    # Build JSON output\n",
    "    resume_json = {\n",
    "        \"resumeData\": {\n",
    "            \"basicDetails\": {\n",
    "                \"name\": {\"value\": name, \"confidence\": name_conf},\n",
    "                \"lastname\": {\"value\": lastname, \"confidence\": lastname_conf},\n",
    "                \"email\": {\"value\": email, \"confidence\": email_conf},\n",
    "                \"phone\": {\"value\": phone, \"confidence\": phone_conf},\n",
    "                \"address\": {\"value\": \"\", \"confidence\": 0.0}\n",
    "            },\n",
    "            \"workExperience\": [],\n",
    "            \"education\": [],\n",
    "            \"skills\": [{\"skill\": {\"value\": s, \"confidence\": round(sim*10, 2)}} for s, sim in found_skills],\n",
    "            \"certifications\": []\n",
    "        },\n",
    "        \"summary\": {\"value\": \"\", \"confidence\": 0.0},\n",
    "        \"htmlContent\": \"\",\n",
    "        \"pdfText\": text\n",
    "    }\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(\"Skills Found (with confidence):\")\n",
    "    for skill, sim in found_skills:\n",
    "        print(f\"{skill}: {sim:.4f}\")\n",
    "    print(\"Job Titles Found (with confidence):\")\n",
    "    for title, sim in found_titles:\n",
    "        print(f\"{title}: {sim:.4f}\")\n",
    "\n",
    "    return resume_json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"C:\\Users\\velam\\OneDrive\\Documents\\pavan's resume.docx\"\n",
    "    skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_skills_parts\"\n",
    "    titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_titles_parts\"\n",
    "\n",
    "    result = process_resume(file_path, skills_folder, titles_folder)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75356eb4",
   "metadata": {},
   "source": [
    "using ollama llama3 with prompt eng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58851b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting PDF pages to images...\n",
      "[INFO] Running OCR on page 1\n",
      "[INFO] Running OCR on page 2\n",
      "[INFO] Running OCR on page 3\n",
      "[INFO] Running OCR on page 4\n",
      "[INFO] Parsing resume with Ollama...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " DIGVIJAYSINGHPARMAR\n",
      "\n",
      "Permanent Address: 40/C, Sangam Nagar, Indore, M.P â€” 452006\n",
      "Present Address:2195 NiralaKunj, Gas AthourityOf India Township\n",
      "Dibiyapur, Auraiya, Uttar Pradesh, |ndia-206244\n",
      "\n",
      "Emait:digv10sp@gmail.com, digvijay10sp@outlook.com\n",
      "Mobile: +91-9662489663\n",
      "\n",
      "in ee Jinkedin.com/in/digvijay-singh-parmar-aabb0953\n",
      "\n",
      "CAREER OBJECTIVE\n",
      "\n",
      "To utilize my ability and knowledge about Safety and Emergency control methods, including\n",
      "the detection of risks to prevent life-threatening harms and damages  \n",
      "\n",
      "{\n",
      "  \"resumeData\": {\n",
      "    \"basicDetails\": {\n",
      "      \"name\": \"Digvijay Singh Parmar\",\n",
      "      \"lastname\": \"\",\n",
      "      \"email\": [\n",
      "        \"digv10sp@gmail.com\",\n",
      "        \"digvijay10sp@outlook.com\"\n",
      "      ],\n",
      "      \"phone\": \"+91-9662489663\"\n",
      "    },\n",
      "    \"workExperience\": [\n",
      "      {\n",
      "        \"company\": \"Government owned GAS ATHOURITY OF INDIA LIMITED (GAIL)\",\n",
      "        \"position\": \"SENIOR OFFICER - FIRE & SAFETY\",\n",
      "        \"duration\": \"6 years\"\n",
      "      },\n",
      "      {\n",
      "        \"company\": \"RELIANCE INDUSTRIES LIMITED (petrochemical Plant) - Dahej-Manufacturing Division in Gujarat, India\",\n",
      "        \"position\": \"MANAGER - FIRE\",\n",
      "        \"duration\": \"2 years\"\n",
      "      },\n",
      "      {\n",
      "        \"company\": \"French MNC BUREAU VERITAS at Noida, India\",\n",
      "        \"position\": \"HSE ENGINEER\",\n",
      "        \"duration\": \"2 years\"\n",
      "      }\n",
      "    ],\n",
      "    \"education\": [\n",
      "      {\n",
      "        \"degree\": \"Bachelor of Engineering in FIRE TECHNOLOGY & SAFETY ENGINEERING\",\n",
      "        \"institution\": \"IES IPS ACADEMY, INDORE\",\n",
      "        \"year\": \"2008-2012\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills\": [],\n",
      "    \"titles\": [\n",
      "      {\n",
      "        \"title\": {\n",
      "          \"value\": \"\",\n",
      "          \"confidence\": 0.0\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"certifications\": [\n",
      "      \"NEBOSH award in Health Safety at Work\",\n",
      "      \"Certificate of proficiency in INDUSTRIAL DISASTER MANAGEMENT from NATIONAL CIVIL DEFENCE COLLAGE, NAGPUR, INDIA.\",\n",
      "      \"IMS Internal Auditor Certificate (ISO 9001, ISO-14001, OHSAS-18001)\",\n",
      "      \"Certificate in INDUSTRIAL SAFETY from NATIONAL SAFETY COUNCIL, INDIA.\",\n",
      "      \"IMIST (International Minimum Industry Safety Training) is an OPITO standard for global Oil and Gas Industry.\"\n",
      "    ]\n",
      "  },\n",
      "  \"summary\": {\n",
      "    \"value\": \"\",\n",
      "    \"confidence\": 0.0\n",
      "  },\n",
      "  \"htmlContent\": \"\",\n",
      "  \"pdfText\": \"DIGVIJAYSINGHPARMAR\\n\\nPermanent Address: 40/C, Sangam Nagar, Indore, M.P \\u2014 452006\\nPresent Address:2195 NiralaKunj, Gas AthourityOf India Township\\nDibiyapur, Auraiya, Uttar Pradesh, |ndia-206244\\n\\nEmait:digv10sp@gmail.com, digvijay10sp@outlook.com\\nMobile: +91-9662489663\\n\\nin ee Jinkedin.com/in/digvijay-singh-parmar-aabb0953\\n\\nCAREER OBJECTIVE\\n\\nTo utilize my ability and knowledge about Safety and Emergency control methods, including\\nthe detection of risks to prevent life-threatening harms and damages to properties caused due\\nto accidents.To work with an organization which can provides good growth prospect, Safe\\nworking environment, and standard operating procedures.\\n\\nSUMMARY\\n\\ne 6years of Experience as Fire Safety professional in Oil and Gas industries.Bachelor of\\nEngineering in FIRE TECHNOLOGY & SAFETY ENGINEERING from IES IPS ACADEMY,\\nINDORE, M.P(2008 - 2012)\\n\\ne Incharge in Firefighting operations and maintenance of fire protection systems\\n\\ne Responsible to ensure availability and operability of emergency vehicles,\\nfirefightingequipment and communication equipment at fire station.\\n\\ne Conducting Periodic inspection testing and maintenance of fire protection system such as:\\n\\nFire water pump house, Hydrant System, Automatic/Manual Medium and High Velocity\\nWater Spray System, Deluge Valve, Fixed and Semi Fixed Foam System/portable fire\\nextinguishers, breathing apparatus, fire vehicles, clean agent system, foam flooding\\nsystem, CO2 Flooding system and DCP flooding System.\\n\\ne Conducting fire protection system trainings, Fire Safety Inspections & Audits/ Risk\\nAssessments.\\n\\ne Reference to Governments statute/national and international requirements, NFPA\\nCodes,IS Codes, OISD, NBC Code, BOCW, Factories Act, OH&S,IMS,ISO 14001 and OHSAS\\n18001.\\n\\nWORKING EXPERIENCE\\n\\n\\u00a9 Total working experience6 years.\\n\\ne Presently working with Government owned GAS ATHOURITY OF INDIA LIMITED\\n(GAIL) -Petrochemical Plant at pata, U.P, India as SENIOR OFFICER - FIRE & SAFETY\\nfrom July2016.\\n\\ne Previously worked with RELIANCE INDUSTRIES LIMITED (petrochemical Plant) -\\nDahej-Manufacturing Division in Gujarat, India as MANAGER -FIRE from September\\n2014.to August 2016 Date (2 years}\\na\\n\\ne Worked with French MNC BUREAU VERITAS at Noida, India as HSE ENGINEERfrom July\\n2012 to August 2014 (2 years}\\n\\nPRESENTWORKING PROFILE\\n\\ne Immediate response to all emergency calls including mock drills and mobilization of\\nrequired resources to, mitigate the emergency.\\n\\ne Conducting Periodic inspection testing and maintenance of fire protection system such\\nas:Fire water pump house, Hydrant System, Automatic/Manual Medium and High\\nVelocity Water Spray System, Deluge Valve, Fixed and Semi Fixed Foam System/portable\\nfire extinguishers, breathing apparatus, fire vehicles, clean agent system, foam flooding\\nsystem, CO2 Flooding system and DCP flooding System.\\n\\ne Preparation of inspection, testing and maintenance reports of fire protection system.\\n\\ne SOP preparation and provide training as per SOP.\\n\\ne Inventory management in Fire & Safety store department through Material master\\ncontrol system, SAP-PM & SAP-MM(such as Purchase Requisitions & Material\\nreservation).\\n\\ne SAP for permit request for maintenance work.\\n\\n\\u00a2 Develop/Evaluate the risk assessment (RA) sheet for the specific job and make necessary\\nmodifications therein.\\n\\n\\u00a2 Provide necessary guidance to shift fire executive during routine shift activities and\\nduring emergency.\\n\\ne Participate in risk assessment sessions at plant level for all critical jobs and insure\\nadequate fire preventions measures during such activates in the plant including plant\\nturnaround.\\n\\ne Carryout resource planning and allocation.\\n\\ne Exhibit incident command and contro! technique for emergency control and mitigation,\\n\\ne Impart training of shift fire personnel and others crewmembers.\\n\\ne Ensure availability of all firefighting equipment/system and communications system on\\nreedy to use basis.\\n\\ne Assure reliability/performance of fire vehicles and jeeps.\\n\\ne Suggest and ensure corrective measure based on the incident analysis.\\n\\ne Ensure the material availability and inventory management of charged store at fire\\nstation.\\n\\ne Carryout fire safety inspection round of the pjant.\\n\\ne Carryout preliminary investigation of incidents and release turnout report on incident.\\n\\ne Organize and conduct daily fire drills and PT.\\n\\ne Ensure compliance with fire safety standards, procedures, practices and checklist.\\n\\ne Maintain log book and other shift related documentations\\n\\ne Preparing safety training schedule, training module and organizing Safety training\\nprogram for employees and contractors in coordination with training department.\\n\\ne Review the adequacy of Fire protection systems in plants and upcoming projects\\nrespectively and recommend enhancement if required.\\n\\ne Fire and Safety audits observation analysis and implementation of critical action points.\\n\\n* Working on the methodologies of Process Safety Management (PSM) and maintaining its\\nhighest standard at site.\\na et oe\\n\\nPREVIOUSWORKING PROFILE\\n\\n* Conducting HSE and Fire Safety Audits of industries power plants high rise buildings,\\nhotels with reference to the client specification / applicable standard and statutes.\\n\\n\\u00a2 Carry out testing and operations of Fire Hydrant System, Smoke & Heat Detection\\nSystem, Automatic and Manual Extinguisher, Gaseous Flooding System, de-smoke\\nsystem.\\n\\ne Conducting regular emergency response exercises & drills to ensure highest Jevel of\\npreparedness in emergency. ;\\n\\ne Maintaining record of drills & providing recommendations for emergency response\\nimprovements.\\n\\ne Identifying activities and processes which are hazardous and undertaking steps for\\ncurtailing associated risks.\\n\\n* Conducting internal audit as per check list to ensure that all operations are executed\\nwithin the set Safety stipulations.\\n\\ne Assuring safe storage, transportation and disposal of hazardous waste and compliance of\\nauthorization conditions. :\\n\\ne Participate and assist in conducting qualitative QRA for all critical jobs and new projects\\non site.\\n\\ne Assist in design and development of HSE training modules with reference to national and\\ninternational standards, good engineering practices and client\\u2019s specific requirements if\\nany.\\n\\ne Ensure the OHSAS, EMS, ISO 14001 & 18001 a documents are always updated.\\n\\ne Rendering technical expertise and advice from on all aspects relating to Safety & Loss\\nprevention.\\n\\nCERTIFICATIONS& EXTRA- CURICULLAR ACTIVITES\\n\\n\\u00a2 NEBOSH award in Health Safety at Work,\\n\\n\\u00a2 Certificate of proficiency in INDUSTRIAL DISASTER MANAGEMENT from NATIONAL\\nCIVIL DEFENCE COLLAGE, NAGPUR, INDIA.\\n\\ne IMS Internal Auditor Certificate (ISO 9001, ISO-14001, OHSAS-18001)\\n\\n\\u00a9 Certificate in INDUSTRIAL SAFETY from NATIONAL SAFETY COUNCIL, INDIA.\\n\\nIMIST (International Minimum Industry Safety Training) is an OPITO standard for\\n\\nglobal Oil and Gas Industry.\\n\\nMaterial master control system, SAP-PM & SAP-MM\\n\\nUndergone 2D+3D Designing certification course in AUTOCAD2012.\\n\\nSuccessful completion of 15 days Training From S.T.1 PVT LTD ,PITHAMPUR\\n\\nParticipated in National Level Seminar: - Second BhartiyaVigyanSammelan 2009.\\n\\nVolunteered at national fire work shop 2009 & 2010.\\n\\ne Achieving Excellent Position in Reliance Physical Test Examination (Half Yearly) -\\nSwimming/Running/Chin-Up/Sit-Up/Push-Up\\n\\neee#e#\\nACADEMIC PURSUITS\\n\\nbites. : eer ee ine: hag, HST e mee Rie Mig, University \\u2018ian\\n(B.E) Fire Technology . \\u2018\\nand Safety IES IPS ACADEMY, INDORE (2008-2012) mane\\nEngineering\\nai R.R.M.B GUJRATI SAMAJ H.S SCHOOL, M.P.B.S.E\\nINDORE BHOPAL\\nx GOVT H $ SCHOOL, VIJAYNAGAR INDORE Lette\\nPERSONALITY TRAIT\\n\\n1 Am dynamic, confident, self-motivated and hard working person and capable to face any\\nchallenges in life am a keen Jearner and always strive hard to gain knowledge . easily mix\\nwith other people and accept additional responsibilities with smile and without hesitation. |\\nam self-driven, tireless worker and 24*7 working personality. | also believe in invincibility and\\nalways ready to face challenges in this globally competitiveenvironment.\\n\\nPERSONAL PROFILE\\n\\nName : Digvijay Singh Parmar\\n\\nFathers Name : Abhay Singh Parmar\\n\\nDate of Birth :10thAug, 1990\\n\\nPassport No : 22945629\\n\\nMarital Status: Married\\n\\nLanguages :well versed in English, Hindi\\n\\nPermanent Address: 40/C, Sangam Nagar, Indore, M.P - 452006\\n\\nPresent Address : 2195 NiralaKunj, GAIL Gaon Township, Dibiyapur, Auraiya, U.P, INDIA-206244\\n\\nSkype id: Digvijay Singh Parmar/digvijay10sp@outlook.com\\nHobbies and interest:Gyming, Swimming, reading books, cooking, travelling,\\nlistening music andmaking new friends.\\n\\nDECLARATION\\n\\n| hereby declare that the above-mentioned information is correct up to my knowledge and | pear the\\nresponsibility for the correctness of the above-mentioned particulars.\\n\\nDigvijay Singh Parmar\\n\"\n",
      "}\n",
      "[INFO] Reading DOCX file...\n",
      "[INFO] Parsing resume with Ollama...\n",
      "\n",
      "===== RESUME PARSE RESULT =====\n",
      "Top 500 Characters of Extracted Text:\n",
      " Velamala Pavan Krishna\n",
      "\n",
      "SUMMARY\n",
      "Passionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\n",
      "\n",
      "Phone:\n",
      "+91 7569637875\n",
      "Email:\n",
      "velamalapavankrishna@gmail.com\n",
      "Address: Visakhapatnam , \n",
      "Andhra Pradesh , India\n",
      "    530046\n",
      "\n",
      "Profile links\n",
      "\n",
      "Linkedin: linkedin.co \n",
      "\n",
      "{\n",
      "  \"resumeData\": {\n",
      "    \"basicDetails\": {\n",
      "      \"name\": \"Velamala Pavan Krishna\",\n",
      "      \"lastname\": \"\",\n",
      "      \"email\": [\n",
      "        \"velamalapavankrishna@gmail.com\"\n",
      "      ],\n",
      "      \"phone\": \"+91 7569637875\"\n",
      "    },\n",
      "    \"workExperience\": [\n",
      "      {\n",
      "        \"company\": \"HuntsJob\",\n",
      "        \"position\": \"Intern\",\n",
      "        \"duration\": \"April-2025 - June 2025\"\n",
      "      },\n",
      "      {\n",
      "        \"company\": \"\",\n",
      "        \"position\": \"HealthCare AI Club Member\",\n",
      "        \"duration\": \"2025 - current\"\n",
      "      }\n",
      "    ],\n",
      "    \"education\": [\n",
      "      {\n",
      "        \"degree\": \"B-Tech in Computer Science and Artificial Intelligence\",\n",
      "        \"institution\": \"Amrita Vishwa Vidyapeetham\",\n",
      "        \"year\": \"2023-2027\"\n",
      "      }\n",
      "    ],\n",
      "    \"skills\": [\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Python\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Java\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"C\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Data Structures and Algorithms\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"OOPS\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"ML fundamental\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Fundementals of AI\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Flask\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"HTML\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"CSS\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Jupyter Notebook\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Matlab\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"VScode\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Git\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Problem-Solving\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Team Collaboration\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Communication\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Critical Thinking\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"skill\": {\n",
      "          \"value\": \"Flexible Adaptive learning\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"titles\": [\n",
      "      {\n",
      "        \"title\": {\n",
      "          \"value\": \"\",\n",
      "          \"confidence\": 10.0\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"certifications\": [\n",
      "      \"Microsoft Certified: Azure AI Fundamentals by coincent\",\n",
      "      \"Python for Data Science, AI & Development by IBM\",\n",
      "      \"HTML, CSS, and Javascript for Web Developers by John Hopkins University\",\n",
      "      \"Generative AI with Large Language Models by DeepLearning.AI\",\n",
      "      \"Using Python to Interact with the Operating System by Google\"\n",
      "    ]\n",
      "  },\n",
      "  \"summary\": {\n",
      "    \"value\": \"\",\n",
      "    \"confidence\": 0.0\n",
      "  },\n",
      "  \"htmlContent\": \"\",\n",
      "  \"pdfText\": \"Velamala Pavan Krishna\\n\\nSUMMARY\\nPassionate computer science and AI student with a strong foundation in machine learning, data analysis, and model building . Eager to explore emerging technologies and apply creative solutions to real-world problems . Committed to continuous learning, collaboration, and making a meaningful impact through AI.\\n\\nPhone:\\n+91 7569637875\\nEmail:\\nvelamalapavankrishna@gmail.com\\nAddress: Visakhapatnam , \\nAndhra Pradesh , India\\n    530046\\n\\nProfile links\\n\\nLinkedin: linkedin.com/in/pavan-krishna-velamala-6a92062b4\\nGithub: https://github.com/W4RG0Dpk\\nEDUCATION\\nB-Tech in Computer Science and Artificial Intelligence | 2023-2027\\nAmrita Vishwa Vidyapeetham, Amritapuri, Kollam, Kerala | GPA : 9.11\\n\\nEXPERIENCE\\nHealthCare AI Club Member | 2025 - current\\nAmrita Vishwa Vidyapeetham \\nWorking on George B. Moody PhysioNet Challenge (team name: HAI_AMRITIANS)\\nIntern at HuntsJob | April-2025 -  June 2025\\n Worked on resume parser \\n\\nCERTIFICATIONS\\nMicrosoft Certified: Azure AI Fundamentals by coincent (2023-2024)\\nPython for Data Science, AI & Development by IBM (July 23, 2024)\\nHTML, CSS, and Javascript for Web Developers by John Hopkins University (Aug 4, 2024)\\nGenerative AI with Large Language Models by DeepLearning.AI (Dec 7, 2024)\\nUsing Python to Interact with the Operating System by Google (Mar 9, 2025)\\n\\nPROJECTS\\nHack assembler in py (from scratch)\\nIris classifier (from scratch)\\nBus reservation system using html ,css, javascript\\nLibrary management system using swing , java\\nChess game-AI\\nCentralized-DNS-with-Proxy-DNS-Server-using-socket-programming\\nLandmark detection using PyTorch\\nMental health chatbot ( Epochon hackathon 2024 )\\nDiabetes prediction using logistic regresison (from scratch)\\nDiesease prediction based on symptoms (ongoing ML project)\\nChagas detection (George B. Moody PhysioNet Challenge) [ongoing]\\n\\nACTIVITIES\\nVolunteered at Amritavarsham 70 & 71.\\nVolunteered for food serving at ICTR-3 (INTERNATIONAL CONFERENCE ON TSUNAMI RISK REDUCTION& RESILIENCE) Seminar.\\n\\nSkills\\nTechnical skills:\\nProgramming : Python, Java, C \\nCore CS: Data Structures and Algorithms , OOPS\\nAI : ML fundamental ,  Fundementals of AI\\nWeb Development: Flask, HTML, CSS\\nDatabase: MySQL\\nTools: Jupyter Notebook, Matlab , VScode , Git\\nSoft skills:\\nProblem-Solving      Team Collaboration \\n Communication       Critical Thinking\\nFlexible Adaptive learning    \"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "\n",
    "# Set your poppler and tesseract paths\n",
    "tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = r\"C:\\Release-24.08.0-0\\poppler-24.08.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"llama3\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"[INFO] Converting PDF pages to images...\")\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "    text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"[INFO] Running OCR on page {i+1}\")\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    print(\"[INFO] Reading DOCX file...\")\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def ollama_parse_resume(text):\n",
    "    # Highly explicit, structured prompt for best accuracy\n",
    "    prompt = f\"\"\"\n",
    "You are a professional resume parsing assistant. Extract the following fields from the resume text below, using only information explicitly present in the text. Do not guess or hallucinate. If a field is missing, use an empty string or empty list.\n",
    "\n",
    "Return ONLY a valid JSON object with this structure:\n",
    "{{\n",
    "  \"basicDetails\": {{\n",
    "    \"name\": \"First name only, as found in text\",\n",
    "    \"lastname\": \"Last name only, as found in text\",\n",
    "    \"email\": [\"all email addresses found, as a list\"],\n",
    "    \"phone\": \"phone number as found\"\n",
    "  }},\n",
    "  \"skills\": [\n",
    "    {{\"skill\": {{\"value\": \"skill name\", \"confidence\": 10.0}}}}\n",
    "  ],\n",
    "  \"titles\": [\n",
    "    {{\"title\": {{\"value\": \"job title\", \"confidence\": 10.0}}}}\n",
    "  ],\n",
    "  \"workExperience\": [\n",
    "    {{\n",
    "      \"company\": \"company name\",\n",
    "      \"position\": \"job title\",\n",
    "      \"duration\": \"duration or years\"\n",
    "    }}\n",
    "  ],\n",
    "  \"education\": [\n",
    "    {{\n",
    "      \"degree\": \"degree name\",\n",
    "      \"institution\": \"institution name\",\n",
    "      \"year\": \"year or range\"\n",
    "    }}\n",
    "  ],\n",
    "  \"certifications\": [\"certification names as list\"]\n",
    "}}\n",
    "\n",
    "Resume Text:\n",
    "{text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_HOST}/api/generate\",\n",
    "            json={\"model\": OLLAMA_MODEL, \"prompt\": prompt, \"stream\": False},\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        # Extract JSON from response\n",
    "        start = result['response'].find('{')\n",
    "        end = result['response'].rfind('}') + 1\n",
    "        parsed = json.loads(result['response'][start:end])\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ollama parsing failed: {e}\")\n",
    "        parsed = {\"error\": str(e), \"raw_response\": result.get('response', '')}\n",
    "    return parsed\n",
    "\n",
    "def process_resume(file_path):\n",
    "    # Extract text from file\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use PDF or DOCX.\")\n",
    "\n",
    "    print(\"[INFO] Parsing resume with Ollama...\")\n",
    "    parsed_data = ollama_parse_resume(text)\n",
    "    resume_json = {\n",
    "        \"resumeData\": {\n",
    "            \"basicDetails\": parsed_data.get(\"basicDetails\", {}),\n",
    "            \"workExperience\": parsed_data.get(\"workExperience\", []),\n",
    "            \"education\": parsed_data.get(\"education\", []),\n",
    "            \"skills\": parsed_data.get(\"skills\", []),\n",
    "            \"titles\": parsed_data.get(\"titles\", []),\n",
    "            \"certifications\": parsed_data.get(\"certifications\", [])\n",
    "        },\n",
    "        \"summary\": {\"value\": \"\", \"confidence\": 0.0},\n",
    "        \"htmlContent\": \"\",\n",
    "        \"pdfText\": text\n",
    "    }\n",
    "\n",
    "    print(\"\\n===== RESUME PARSE RESULT =====\")\n",
    "    print(\"Top 500 Characters of Extracted Text:\\n\", text[:500], \"\\n\")\n",
    "    print(json.dumps(resume_json, indent=2))\n",
    "    return resume_json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    file_path2=r\"C:\\amrita_uni\\Projects\\asmacs internship\\Imgdownload2.pdf\"\n",
    "    result2= process_resume(file_path2)\n",
    "    file_path = r\"C:\\Users\\velam\\OneDrive\\Documents\\pavan's resume.docx\"\n",
    "    result = process_resume(file_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdffaf",
   "metadata": {},
   "source": [
    "with skills and titles finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b99b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "\n",
    "def load_keywords_from_folder(folder_path):\n",
    "    keywords = set()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    kw_clean = line.strip().lower()\n",
    "                    if kw_clean:\n",
    "                        keywords.add(kw_clean)\n",
    "        elif filename.endswith(\".csv\"):\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    for item in row:\n",
    "                        kw_clean = item.strip().lower()\n",
    "                        if kw_clean:\n",
    "                            keywords.add(kw_clean)\n",
    "    return list(keywords)\n",
    "\n",
    "# Set your paths\n",
    "skills_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_skills_parts\"\n",
    "titles_folder = r\"C:\\amrita_uni\\Projects\\asmacs internship\\recognition of skills and title\\unique_job_titles_parts\"\n",
    "\n",
    "skills = load_keywords_from_folder(skills_folder)\n",
    "titles = load_keywords_from_folder(titles_folder)\n",
    "\n",
    "# Create training data\n",
    "examples = []\n",
    "for skill in skills:\n",
    "    examples.append({\n",
    "        \"prompt\": f\"Extract skills and titles: The candidate is skilled in {skill}.\",\n",
    "        \"completion\": json.dumps({\"skills\": [skill], \"titles\": []})\n",
    "    })\n",
    "\n",
    "for title in titles:\n",
    "    examples.append({\n",
    "        \"prompt\": f\"Extract skills and titles: The candidate worked as a {title}.\",\n",
    "        \"completion\": json.dumps({\"skills\": [], \"titles\": [title]})\n",
    "    })\n",
    "\n",
    "# Add 10,000 mixed examples\n",
    "for _ in range(10000):\n",
    "    skill = random.choice(skills)\n",
    "    title = random.choice(titles)\n",
    "    examples.append({\n",
    "        \"prompt\": f\"Extract skills and titles: Worked as {title} with {skill} skills.\",\n",
    "        \"completion\": json.dumps({\"skills\": [skill], \"titles\": [title]})\n",
    "    })\n",
    "\n",
    "# Save training data\n",
    "with open(\"finetune_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
